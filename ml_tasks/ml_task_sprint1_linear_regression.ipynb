{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf597042-4353-499c-8def-e495305ae9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636b5c8-86c8-4aa0-bb09-f267895eade3",
   "metadata": {},
   "source": [
    "# Linear Regression Stratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "57f1438b-8ea3-4a8c-a4aa-41ce69dfd45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_iter, lr, no_bias, verbose): \n",
    "        \n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = no_bias\n",
    "        self.verbose = verbose\n",
    "     \n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "      \n",
    "\n",
    "    # 問題6（学習と推定）\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.bias == True:\n",
    "            bias = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((bias, X))\n",
    "            if X_val is not None:\n",
    "                bias = np.ones((X_val.shape[0], 1))\n",
    "                X_val = np.hstack((bias, X_val))\n",
    "            self.coef_ = np.random.rand(X.shape[1])\n",
    "            self.coef_ = self.coef_.reshape(X.shape[1], 1)\n",
    "    \n",
    "\n",
    "        for epoch in range(self.iter):\n",
    "            y_pred = self._linear_hypothesis(X)\n",
    "            self.loss[epoch] = np.mean((y-y_pred)**2)\n",
    "            \n",
    "            if X_val is not None:\n",
    "                pred_val = self._linear_hypothesis(X_val)\n",
    "                self.val_loss[epoch] = np.mean((y_val-pred_val)**2)\n",
    "                \n",
    "            self.coef_ = self._gradient_descent(X, (y_pred-y))\n",
    "           \n",
    "            if self.verbose == True:\n",
    "                print('{}-th epoch train loss {}'.format(epoch, self.loss[epoch]))\n",
    "                if X_val is not None:\n",
    "                    print('{}-th epoch val loss {}'.format(epoch, self.val_loss[epoch] ))\n",
    "\n",
    "\n",
    "    # 問題1\n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        仮定関数の出力を計算する\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "        Returns\n",
    "        -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        線形の仮定関数による推定結果\n",
    "        \"\"\"\n",
    "        pred = X @ self.coef_\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    # 問題2\n",
    "    def _gradient_descent(self, X, error):\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "            gradient = error*X[:, i]\n",
    "            self.coef_[i, :] = self.coef_[i, :] - self.lr * np.mean(gradient)\n",
    "\n",
    "        return self.coef_\n",
    "        \n",
    "\n",
    "    # 問題3\n",
    "    def predict(self, X):\n",
    "        if self.bias == True:\n",
    "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
    "            X = np.hstack([bias, X])\n",
    "        pred_y = self._linear_hypothesis(X)\n",
    "        return pred_y\n",
    "\n",
    "    # 問題4\n",
    "    def _mse(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        平均二乗誤差の計算\n",
    "        \"\"\"\n",
    "        mse = np.mean((y-y_pred)**2)\n",
    "        \n",
    "        return mse\n",
    "\n",
    "    # 問題5\n",
    "    def _loss_func(self, pred, y):\n",
    "        \"\"\"\n",
    "        損失関数\n",
    "        \"\"\"\n",
    "        loss = self._mse(pred, y)/2\n",
    "\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad5217-4c03-4193-b352-b87cd5152bc4",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "78c13bde-f2b2-42aa-9f70-27954c747b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "dataset = pd.read_csv(\"../data/house_price/train.csv\")\n",
    "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
    "y = dataset.loc[:, ['SalePrice']]\n",
    "X = X.values\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "y = np.log(y.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ef23b8d7-cdaf-46a7-b5ab-f5fa155808ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th epoch train loss 128.81119395373\n",
      "0-th epoch val loss 128.8944717909771\n",
      "1-th epoch train loss 124.79837542188821\n",
      "1-th epoch val loss 124.88847301485383\n",
      "2-th epoch train loss 120.91096587802342\n",
      "2-th epoch val loss 121.00754124892734\n",
      "3-th epoch train loss 117.14505438859759\n",
      "3-th epoch val loss 117.24777823631256\n",
      "4-th epoch train loss 113.49685185368372\n",
      "4-th epoch val loss 113.6054071273481\n",
      "5-th epoch train loss 109.96268721364136\n",
      "5-th epoch val loss 110.07676870004104\n",
      "6-th epoch train loss 106.53900377386653\n",
      "6-th epoch val loss 106.65831769814923\n",
      "7-th epoch train loss 103.22235564394092\n",
      "7-th epoch val loss 103.3466192832405\n",
      "8-th epoch train loss 100.00940428762043\n",
      "8-th epoch val loss 100.13834559718174\n",
      "9-th epoch train loss 96.89691518021284\n",
      "9-th epoch val loss 97.03027243162073\n",
      "10-th epoch train loss 93.88175457000278\n",
      "10-th epoch val loss 94.01927600113129\n",
      "11-th epoch train loss 90.96088634048569\n",
      "11-th epoch val loss 91.10232981679529\n",
      "12-th epoch train loss 88.1313689702731\n",
      "12-th epoch val loss 88.27650165709579\n",
      "13-th epoch train loss 85.3903525876296\n",
      "13-th epoch val loss 85.53895063309302\n",
      "14-th epoch train loss 82.73507611669622\n",
      "14-th epoch val loss 82.88692434494861\n",
      "15-th epoch train loss 80.16286451254653\n",
      "15-th epoch val loss 80.31775612695536\n",
      "16-th epoch train loss 77.67112608231093\n",
      "16-th epoch val loss 77.82886237831795\n",
      "17-th epoch train loss 75.25734988969013\n",
      "17-th epoch val loss 75.41773997701594\n",
      "18-th epoch train loss 72.91910324026269\n",
      "18-th epoch val loss 73.08196377416323\n",
      "19-th epoch train loss 70.6540292450718\n",
      "19-th epoch val loss 70.819184166359\n",
      "20-th epoch train loss 68.45984446005536\n",
      "20-th epoch val loss 68.62712474360225\n",
      "21-th epoch train loss 66.33433659895829\n",
      "21-th epoch val loss 66.5035800104193\n",
      "22-th epoch train loss 64.27536231744043\n",
      "22-th epoch val loss 64.44641317792437\n",
      "23-th epoch train loss 62.28084506616408\n",
      "23-th epoch val loss 62.45355402460669\n",
      "24-th epoch train loss 60.34877301071418\n",
      "24-th epoch val loss 60.52299682370472\n",
      "25-th epoch train loss 58.47719701627107\n",
      "25-th epoch val loss 58.652798335094985\n",
      "26-th epoch train loss 56.66422869502038\n",
      "26-th epoch val loss 56.84107585968812\n",
      "27-th epoch train loss 54.90803851434765\n",
      "27-th epoch val loss 55.08600535438621\n",
      "28-th epoch train loss 53.20685396392549\n",
      "28-th epoch val loss 53.38581960571702\n",
      "29-th epoch train loss 51.55895777986049\n",
      "29-th epoch val loss 51.73880646031863\n",
      "30-th epoch train loss 49.96268622412387\n",
      "30-th epoch val loss 50.143307110505155\n",
      "31-th epoch train loss 48.41642741754516\n",
      "31-th epoch val loss 48.59771443319946\n",
      "32-th epoch train loss 46.91861972470174\n",
      "32-th epoch val loss 47.10047138057167\n",
      "33-th epoch train loss 45.46775018908922\n",
      "33-th epoch val loss 45.65006942077436\n",
      "34-th epoch train loss 44.06235301700731\n",
      "34-th epoch val loss 44.2450470272152\n",
      "35-th epoch train loss 42.70100810864518\n",
      "35-th epoch val loss 42.88398821485647\n",
      "36-th epoch train loss 41.38233963489721\n",
      "36-th epoch val loss 41.56552112207774\n",
      "37-th epoch train loss 40.105014658485544\n",
      "37-th epoch val loss 40.28831663668355\n",
      "38-th epoch train loss 38.867741798010684\n",
      "38-th epoch val loss 39.0510870646824\n",
      "39-th epoch train loss 37.669269933593746\n",
      "39-th epoch val loss 37.85258484050555\n",
      "40-th epoch train loss 36.5083869528159\n",
      "40-th epoch val loss 36.691601277376066\n",
      "41-th epoch train loss 35.38391853570064\n",
      "41-th epoch val loss 35.56696535657833\n",
      "42-th epoch train loss 34.294726977523894\n",
      "42-th epoch val loss 34.477542554417454\n",
      "43-th epoch train loss 33.239710048274155\n",
      "43-th epoch val loss 33.42223370569531\n",
      "44-th epoch train loss 32.21779988762218\n",
      "44-th epoch val loss 32.39997390256687\n",
      "45-th epoch train loss 31.22796193429488\n",
      "45-th epoch val loss 31.409731427675457\n",
      "46-th epoch train loss 30.269193888782493\n",
      "46-th epoch val loss 30.450506720500158\n",
      "47-th epoch train loss 29.340524708341505\n",
      "47-th epoch val loss 29.52133137588157\n",
      "48-th epoch train loss 28.44101363328811\n",
      "48-th epoch val loss 28.62126717372439\n",
      "49-th epoch train loss 27.56974924360823\n",
      "49-th epoch val loss 27.749405138906518\n",
      "50-th epoch train loss 26.725848544940344\n",
      "50-th epoch val loss 26.90486463045444\n",
      "51-th epoch train loss 25.90845608301698\n",
      "51-th epoch val loss 26.08679245907401\n",
      "52-th epoch train loss 25.11674308567895\n",
      "52-th epoch val loss 25.29436203215408\n",
      "53-th epoch train loss 24.349906631604075\n",
      "53-th epoch val loss 24.526772525387845\n",
      "54-th epoch train loss 23.60716884491886\n",
      "54-th epoch val loss 23.783248080183455\n",
      "55-th epoch train loss 22.887776114887448\n",
      "55-th epoch val loss 23.063037026061224\n",
      "56-th epoch train loss 22.19099833989725\n",
      "56-th epoch val loss 22.36541112725961\n",
      "57-th epoch train loss 21.51612819498496\n",
      "57-th epoch val loss 21.689664852796607\n",
      "58-th epoch train loss 20.86248042217017\n",
      "58-th epoch val loss 21.035114669256348\n",
      "59-th epoch train loss 20.229391142886634\n",
      "59-th epoch val loss 20.401098355593614\n",
      "60-th epoch train loss 19.61621719182336\n",
      "60-th epoch val loss 19.786974339271033\n",
      "61-th epoch train loss 19.022335471509017\n",
      "61-th epoch val loss 19.192121053064803\n",
      "62-th epoch train loss 18.447142326993962\n",
      "62-th epoch val loss 18.61593631189569\n",
      "63-th epoch train loss 17.890052940004313\n",
      "63-th epoch val loss 18.057836709062027\n",
      "64-th epoch train loss 17.350500741961863\n",
      "64-th epoch val loss 17.517257031270688\n",
      "65-th epoch train loss 16.827936845282597\n",
      "65-th epoch val loss 16.993649691881043\n",
      "66-th epoch train loss 16.321829492384847\n",
      "66-th epoch val loss 16.486484181794914\n",
      "67-th epoch train loss 15.831663521855697\n",
      "67-th epoch val loss 15.995246537443302\n",
      "68-th epoch train loss 15.356939851241586\n",
      "68-th epoch val loss 15.519438825337655\n",
      "69-th epoch train loss 14.897174975945601\n",
      "69-th epoch val loss 15.058578642670161\n",
      "70-th epoch train loss 14.45190048372999\n",
      "70-th epoch val loss 14.612198633463441\n",
      "71-th epoch train loss 14.0206625843382\n",
      "71-th epoch val loss 14.179846019785654\n",
      "72-th epoch train loss 13.603021653765692\n",
      "72-th epoch val loss 13.761082147562075\n",
      "73-th epoch train loss 13.198551792723524\n",
      "73-th epoch val loss 13.355482046528756\n",
      "74-th epoch train loss 12.806840398852874\n",
      "74-th epoch val loss 12.962634003888093\n",
      "75-th epoch train loss 12.427487752262467\n",
      "75-th epoch val loss 12.582139151239794\n",
      "76-th epoch train loss 12.060106613974053\n",
      "76-th epoch val loss 12.21361106437396\n",
      "77-th epoch train loss 11.704321836874175\n",
      "77-th epoch val loss 11.856675375525972\n",
      "78-th epoch train loss 11.359769988782885\n",
      "78-th epoch val loss 11.510969397705221\n",
      "79-th epoch train loss 11.02609898726209\n",
      "79-th epoch val loss 11.176141760721832\n",
      "80-th epoch train loss 10.702967745798196\n",
      "80-th epoch val loss 10.851852058547305\n",
      "81-th epoch train loss 10.390045831004805\n",
      "81-th epoch val loss 10.537770507656159\n",
      "82-th epoch train loss 10.087013130502504\n",
      "82-th epoch val loss 10.233577616006867\n",
      "83-th epoch train loss 9.793559531143222\n",
      "83-th epoch val loss 9.93896386233078\n",
      "84-th epoch train loss 9.509384607257214\n",
      "84-th epoch val loss 9.653629385408237\n",
      "85-th epoch train loss 9.23419731861054\n",
      "85-th epoch val loss 9.377283683020934\n",
      "86-th epoch train loss 8.96771571777079\n",
      "86-th epoch val loss 9.109645320279308\n",
      "87-th epoch train loss 8.709666666588095\n",
      "87-th epoch val loss 8.850441647033163\n",
      "88-th epoch train loss 8.459785561507656\n",
      "88-th epoch val loss 8.599408524082694\n",
      "89-th epoch train loss 8.21781606743882\n",
      "89-th epoch val loss 8.356290057916\n",
      "90-th epoch train loss 7.983509859914316\n",
      "90-th epoch val loss 8.120838343707645\n",
      "91-th epoch train loss 7.756626375281497\n",
      "91-th epoch val loss 7.892813216321089\n",
      "92-th epoch train loss 7.536932568675587\n",
      "92-th epoch val loss 7.671982009065817\n",
      "93-th epoch train loss 7.324202679532562\n",
      "93-th epoch val loss 7.458119319967764\n",
      "94-th epoch train loss 7.118218004406948\n",
      "94-th epoch val loss 7.251006785319109\n",
      "95-th epoch train loss 6.918766676867136\n",
      "95-th epoch val loss 7.050432860280899\n",
      "96-th epoch train loss 6.725643454247744\n",
      "96-th epoch val loss 6.856192606318815\n",
      "97-th epoch train loss 6.538649511045657\n",
      "97-th epoch val loss 6.668087485259477\n",
      "98-th epoch train loss 6.357592238752756\n",
      "98-th epoch val loss 6.485925159761065\n",
      "99-th epoch train loss 6.182285051925017\n",
      "99-th epoch val loss 6.309519299998668\n",
      "100-th epoch train loss 6.012547200293724\n",
      "100-th epoch val loss 6.138689396370765\n",
      "101-th epoch train loss 5.84820358673073\n",
      "101-th epoch val loss 5.973260578039512\n",
      "102-th epoch train loss 5.689084590885418\n",
      "102-th epoch val loss 5.813063437123104\n",
      "103-th epoch train loss 5.535025898316797\n",
      "103-th epoch val loss 5.657933858364278\n",
      "104-th epoch train loss 5.385868334949641\n",
      "104-th epoch val loss 5.507712854104528\n",
      "105-th epoch train loss 5.2414577066888\n",
      "105-th epoch val loss 5.362246404398707\n",
      "106-th epoch train loss 5.1016446440311976\n",
      "106-th epoch val loss 5.221385302110148\n",
      "107-th epoch train loss 4.9662844515197495\n",
      "107-th epoch val loss 5.084985002831066\n",
      "108-th epoch train loss 4.835236961888551\n",
      "108-th epoch val loss 4.952905479478161\n",
      "109-th epoch train loss 4.708366394753152\n",
      "109-th epoch val loss 4.825011081417745\n",
      "110-th epoch train loss 4.585541219704444\n",
      "110-th epoch val loss 4.701170397979438\n",
      "111-th epoch train loss 4.46663402366901\n",
      "111-th epoch val loss 4.5812561262217555\n",
      "112-th epoch train loss 4.351521382403055\n",
      "112-th epoch val loss 4.465144942817225\n",
      "113-th epoch train loss 4.240083735991228\n",
      "113-th epoch val loss 4.352717379928741\n",
      "114-th epoch train loss 4.132205268225591\n",
      "114-th epoch val loss 4.243857704952926\n",
      "115-th epoch train loss 4.027773789743911\n",
      "115-th epoch val loss 4.138453804010045\n",
      "116-th epoch train loss 3.926680624810182\n",
      "116-th epoch val loss 4.036397069063869\n",
      "117-th epoch train loss 3.8288205016239845\n",
      "117-th epoch val loss 3.9375822885584024\n",
      "118-th epoch train loss 3.73409144604874\n",
      "118-th epoch val loss 3.8419075414620383\n",
      "119-th epoch train loss 3.6423946786524093\n",
      "119-th epoch val loss 3.7492740946129723\n",
      "120-th epoch train loss 3.5536345149574706\n",
      "120-th epoch val loss 3.6595863032631417\n",
      "121-th epoch train loss 3.4677182688002195\n",
      "121-th epoch val loss 3.5727515147210513\n",
      "122-th epoch train loss 3.384556158702544\n",
      "122-th epoch val loss 3.4886799749970088\n",
      "123-th epoch train loss 3.304061217162376\n",
      "123-th epoch val loss 3.4072847383572755\n",
      "124-th epoch train loss 3.2261492027718788\n",
      "124-th epoch val loss 3.3284815796965517\n",
      "125-th epoch train loss 3.1507385150753375\n",
      "125-th epoch val loss 3.2521889096410384\n",
      "126-th epoch train loss 3.077750112081378\n",
      "126-th epoch val loss 3.1783276922970347\n",
      "127-th epoch train loss 3.007107430346883\n",
      "127-th epoch val loss 3.106821365562704\n",
      "128-th epoch train loss 2.938736307552462\n",
      "128-th epoch val loss 3.0375957639231728\n",
      "129-th epoch train loss 2.872564907491896\n",
      "129-th epoch val loss 2.9705790436516413\n",
      "130-th epoch train loss 2.80852364740036\n",
      "130-th epoch val loss 2.905701610341583\n",
      "131-th epoch train loss 2.7465451275485644\n",
      "131-th epoch val loss 2.842896048697426\n",
      "132-th epoch train loss 2.6865640630322343\n",
      "132-th epoch val loss 2.782097054513386\n",
      "133-th epoch train loss 2.6285172176885574\n",
      "133-th epoch val loss 2.7232413687723183\n",
      "134-th epoch train loss 2.5723433400733264\n",
      "134-th epoch val loss 2.6662677137985553\n",
      "135-th epoch train loss 2.517983101434589\n",
      "135-th epoch val loss 2.611116731400761\n",
      "136-th epoch train loss 2.4653790356206113\n",
      "136-th epoch val loss 2.5577309229428247\n",
      "137-th epoch train loss 2.414475480861905\n",
      "137-th epoch val loss 2.50605459128276\n",
      "138-th epoch train loss 2.365218523368923\n",
      "138-th epoch val loss 2.4560337845214266\n",
      "139-th epoch train loss 2.317555942688869\n",
      "139-th epoch val loss 2.4076162415046887\n",
      "140-th epoch train loss 2.2714371587668087\n",
      "140-th epoch val loss 2.360751339024424\n",
      "141-th epoch train loss 2.2268131806580063\n",
      "141-th epoch val loss 2.31539004066547\n",
      "142-th epoch train loss 2.1836365568400153\n",
      "142-th epoch val loss 2.2714848472472178\n",
      "143-th epoch train loss 2.141861327074702\n",
      "143-th epoch val loss 2.228989748810217\n",
      "144-th epoch train loss 2.1014429757719157\n",
      "144-th epoch val loss 2.1878601780996547\n",
      "145-th epoch train loss 2.0623383868079923\n",
      "145-th epoch val loss 2.1480529654990757\n",
      "146-th epoch train loss 2.0245057997537987\n",
      "146-th epoch val loss 2.109526295369195\n",
      "147-th epoch train loss 1.9879047674683694\n",
      "147-th epoch val loss 2.072239663748013\n",
      "148-th epoch train loss 1.9524961150156084\n",
      "148-th epoch val loss 2.0361538373698487\n",
      "149-th epoch train loss 1.918241899862807\n",
      "149-th epoch val loss 2.001230813962189\n",
      "150-th epoch train loss 1.8851053733210692\n",
      "150-th epoch val loss 1.967433783780573\n",
      "151-th epoch train loss 1.853050943188912\n",
      "151-th epoch val loss 1.934727092342932\n",
      "152-th epoch train loss 1.8220441375615652\n",
      "152-th epoch val loss 1.9030762043260072\n",
      "153-th epoch train loss 1.792051569769656\n",
      "153-th epoch val loss 1.8724476685876859\n",
      "154-th epoch train loss 1.7630409044120534\n",
      "154-th epoch val loss 1.842809084280134\n",
      "155-th epoch train loss 1.7349808244488139\n",
      "155-th epoch val loss 1.8141290680197824\n",
      "156-th epoch train loss 1.707840999321154\n",
      "156-th epoch val loss 1.7863772220812193\n",
      "157-th epoch train loss 1.6815920540664686\n",
      "157-th epoch val loss 1.7595241035830982\n",
      "158-th epoch train loss 1.6562055393973771\n",
      "158-th epoch val loss 1.7335411946351587\n",
      "159-th epoch train loss 1.6316539027147432\n",
      "159-th epoch val loss 1.7084008734164127\n",
      "160-th epoch train loss 1.6079104600255727\n",
      "160-th epoch val loss 1.6840763861554853\n",
      "161-th epoch train loss 1.5849493687375829\n",
      "161-th epoch val loss 1.6605418199850126\n",
      "162-th epoch train loss 1.5627456013031116\n",
      "162-th epoch val loss 1.6377720766428454\n",
      "163-th epoch train loss 1.5412749196858937\n",
      "163-th epoch val loss 1.6157428469936892\n",
      "164-th epoch train loss 1.5205138506250766\n",
      "164-th epoch val loss 1.5944305863456194\n",
      "165-th epoch train loss 1.5004396616715936\n",
      "165-th epoch val loss 1.5738124905366946\n",
      "166-th epoch train loss 1.481030337972833\n",
      "166-th epoch val loss 1.5538664727676719\n",
      "167-th epoch train loss 1.4622645597822899\n",
      "167-th epoch val loss 1.5345711411575937\n",
      "168-th epoch train loss 1.4441216806715773\n",
      "168-th epoch val loss 1.515905776999699\n",
      "169-th epoch train loss 1.4265817064229136\n",
      "169-th epoch val loss 1.4978503136958494\n",
      "170-th epoch train loss 1.4096252745808726\n",
      "170-th epoch val loss 1.4803853163483218\n",
      "171-th epoch train loss 1.3932336346428429\n",
      "171-th epoch val loss 1.4634919619884896\n",
      "172-th epoch train loss 1.377388628868271\n",
      "172-th epoch val loss 1.4471520204225272\n",
      "173-th epoch train loss 1.3620726736874242\n",
      "173-th epoch val loss 1.4313478356749405\n",
      "174-th epoch train loss 1.347268741690942\n",
      "174-th epoch val loss 1.4160623080112524\n",
      "175-th epoch train loss 1.332960344182105\n",
      "175-th epoch val loss 1.4012788765218296\n",
      "176-th epoch train loss 1.3191315142742488\n",
      "176-th epoch val loss 1.3869815022493412\n",
      "177-th epoch train loss 1.3057667905163344\n",
      "177-th epoch val loss 1.373154651842913\n",
      "178-th epoch train loss 1.2928512010302116\n",
      "178-th epoch val loss 1.3597832817225641\n",
      "179-th epoch train loss 1.280370248143602\n",
      "179-th epoch val loss 1.3468528227380125\n",
      "180-th epoch train loss 1.268309893503361\n",
      "180-th epoch val loss 1.334349165306451\n",
      "181-th epoch train loss 1.256656543654027\n",
      "181-th epoch val loss 1.3222586450143556\n",
      "182-th epoch train loss 1.2453970360671578\n",
      "182-th epoch val loss 1.3105680286688715\n",
      "183-th epoch train loss 1.2345186256073828\n",
      "183-th epoch val loss 1.2992645007847494\n",
      "184-th epoch train loss 1.2240089714215623\n",
      "184-th epoch val loss 1.2883356504932686\n",
      "185-th epoch train loss 1.2138561242378476\n",
      "185-th epoch val loss 1.2777694588599835\n",
      "186-th epoch train loss 1.2040485140618644\n",
      "186-th epoch val loss 1.267554286598554\n",
      "187-th epoch train loss 1.1945749382576216\n",
      "187-th epoch val loss 1.2576788621683037\n",
      "188-th epoch train loss 1.1854245500011589\n",
      "188-th epoch val loss 1.2481322702435584\n",
      "189-th epoch train loss 1.1765868470952883\n",
      "189-th epoch val loss 1.2389039405431534\n",
      "190-th epoch train loss 1.1680516611341822\n",
      "190-th epoch val loss 1.2299836370088995\n",
      "191-th epoch train loss 1.1598091470068803\n",
      "191-th epoch val loss 1.2213614473221146\n",
      "192-th epoch train loss 1.1518497727291448\n",
      "192-th epoch val loss 1.213027772747687\n",
      "193-th epoch train loss 1.1441643095934257\n",
      "193-th epoch val loss 1.2049733182954536\n",
      "194-th epoch train loss 1.136743822627003\n",
      "194-th epoch val loss 1.197189083189005\n",
      "195-th epoch train loss 1.1295796613486915\n",
      "195-th epoch val loss 1.1896663516323203\n",
      "196-th epoch train loss 1.1226634508147937\n",
      "196-th epoch val loss 1.1823966838649538\n",
      "197-th epoch train loss 1.1159870829452758\n",
      "197-th epoch val loss 1.1753719074967757\n",
      "198-th epoch train loss 1.1095427081214153\n",
      "198-th epoch val loss 1.1685841091135372\n",
      "199-th epoch train loss 1.103322727046458\n",
      "199-th epoch val loss 1.162025626144828\n",
      "200-th epoch train loss 1.0973197828610646\n",
      "200-th epoch val loss 1.1556890389862347\n",
      "201-th epoch train loss 1.0915267535056026\n",
      "201-th epoch val loss 1.149567163367769\n",
      "202-th epoch train loss 1.0859367443215815\n",
      "202-th epoch val loss 1.1436530429608993\n",
      "203-th epoch train loss 1.0805430808847503\n",
      "203-th epoch val loss 1.1379399422167187\n",
      "204-th epoch train loss 1.075339302062648\n",
      "204-th epoch val loss 1.1324213394280687\n",
      "205-th epoch train loss 1.0703191532895833\n",
      "205-th epoch val loss 1.1270909200086052\n",
      "206-th epoch train loss 1.065476580052262\n",
      "206-th epoch val loss 1.1219425699820622\n",
      "207-th epoch train loss 1.0608057215794862\n",
      "207-th epoch val loss 1.1169703696751365\n",
      "208-th epoch train loss 1.0563009047295515\n",
      "208-th epoch val loss 1.1121685876076581\n",
      "209-th epoch train loss 1.0519566380691732\n",
      "209-th epoch val loss 1.1075316745738786\n",
      "210-th epoch train loss 1.047767606137951\n",
      "210-th epoch val loss 1.1030542579089193\n",
      "211-th epoch train loss 1.0437286638925931\n",
      "211-th epoch val loss 1.0987311359346028\n",
      "212-th epoch train loss 1.0398348313252728\n",
      "212-th epoch val loss 1.0945572725790715\n",
      "213-th epoch train loss 1.0360812882506856\n",
      "213-th epoch val loss 1.090527792164765\n",
      "214-th epoch train loss 1.032463369256541\n",
      "214-th epoch val loss 1.0866379743595103\n",
      "215-th epoch train loss 1.0289765588123698\n",
      "215-th epoch val loss 1.0828832492856257\n",
      "216-th epoch train loss 1.025616486531725\n",
      "216-th epoch val loss 1.079259192782117\n",
      "217-th epoch train loss 1.0223789225829596\n",
      "217-th epoch val loss 1.0757615218151804\n",
      "218-th epoch train loss 1.0192597732439586\n",
      "218-th epoch val loss 1.072386090032384\n",
      "219-th epoch train loss 1.0162550765963163\n",
      "219-th epoch val loss 1.069128883456049\n",
      "220-th epoch train loss 1.0133609983546117\n",
      "220-th epoch val loss 1.0659860163114816\n",
      "221-th epoch train loss 1.0105738278265484\n",
      "221-th epoch val loss 1.0629537269858413\n",
      "222-th epoch train loss 1.0078899739998808\n",
      "222-th epoch val loss 1.0600283741135808\n",
      "223-th epoch train loss 1.0053059617521602\n",
      "223-th epoch val loss 1.057206432784488\n",
      "224-th epoch train loss 1.002818428179461\n",
      "224-th epoch val loss 1.0544844908705224\n",
      "225-th epoch train loss 1.0004241190403713\n",
      "225-th epoch val loss 1.051859245467721\n",
      "226-th epoch train loss 0.9981198853116395\n",
      "226-th epoch val loss 1.0493274994495865\n",
      "227-th epoch train loss 0.9959026798519974\n",
      "227-th epoch val loss 1.0468861581284787\n",
      "228-th epoch train loss 0.9937695541707646\n",
      "228-th epoch val loss 1.0445322260216348\n",
      "229-th epoch train loss 0.9917176552979695\n",
      "229-th epoch val loss 1.0422628037185504\n",
      "230-th epoch train loss 0.9897442227528052\n",
      "230-th epoch val loss 1.0400750848465554\n",
      "231-th epoch train loss 0.9878465856073523\n",
      "231-th epoch val loss 1.037966353131521\n",
      "232-th epoch train loss 0.9860221596425817\n",
      "232-th epoch val loss 1.035933979550723\n",
      "233-th epoch train loss 0.9842684445937597\n",
      "233-th epoch val loss 1.0339754195749826\n",
      "234-th epoch train loss 0.9825830214824506\n",
      "234-th epoch val loss 1.0320882104973\n",
      "235-th epoch train loss 0.9809635500324206\n",
      "235-th epoch val loss 1.030269968845276\n",
      "236-th epoch train loss 0.979407766166801\n",
      "236-th epoch val loss 1.0285183878747024\n",
      "237-th epoch train loss 0.9779134795839861\n",
      "237-th epoch val loss 1.0268312351417856\n",
      "238-th epoch train loss 0.9764785714097911\n",
      "238-th epoch val loss 1.0252063501515547\n",
      "239-th epoch train loss 0.9751009919234875\n",
      "239-th epoch val loss 1.0236416420800543\n",
      "240-th epoch train loss 0.9737787583554078\n",
      "240-th epoch val loss 1.0221350875680433\n",
      "241-th epoch train loss 0.9725099527538711\n",
      "241-th epoch val loss 1.0206847285839367\n",
      "242-th epoch train loss 0.971292719919269\n",
      "242-th epoch val loss 1.019288670353847\n",
      "243-th epoch train loss 0.9701252654032043\n",
      "243-th epoch val loss 1.0179450793566198\n",
      "244-th epoch train loss 0.9690058535706494\n",
      "244-th epoch val loss 1.0166521813818319\n",
      "245-th epoch train loss 0.9679328057231482\n",
      "245-th epoch val loss 1.0154082596487841\n",
      "246-th epoch train loss 0.9669044982811518\n",
      "246-th epoch val loss 1.0142116529845806\n",
      "247-th epoch train loss 0.9659193610236366\n",
      "247-th epoch val loss 1.013060754059455\n",
      "248-th epoch train loss 0.9649758753832148\n",
      "248-th epoch val loss 1.0119540076775422\n",
      "249-th epoch train loss 0.9640725727949886\n",
      "249-th epoch val loss 1.0108899091213661\n",
      "250-th epoch train loss 0.9632080330974758\n",
      "250-th epoch val loss 1.00986700254837\n",
      "251-th epoch train loss 0.9623808829839737\n",
      "251-th epoch val loss 1.008883879437854\n",
      "252-th epoch train loss 0.9615897945027732\n",
      "252-th epoch val loss 1.007939177086742\n",
      "253-th epoch train loss 0.9608334836047016\n",
      "253-th epoch val loss 1.007031577152659\n",
      "254-th epoch train loss 0.9601107087365082\n",
      "254-th epoch val loss 1.006159804242834\n",
      "255-th epoch train loss 0.9594202694786558\n",
      "255-th epoch val loss 1.0053226245473899\n",
      "256-th epoch train loss 0.9587610052261222\n",
      "256-th epoch val loss 1.0045188445156414\n",
      "257-th epoch train loss 0.9581317939108728\n",
      "257-th epoch val loss 1.003747309574049\n",
      "258-th epoch train loss 0.9575315507646878\n",
      "258-th epoch val loss 1.003006902884524\n",
      "259-th epoch train loss 0.9569592271210878\n",
      "259-th epoch val loss 1.0022965441418303\n",
      "260-th epoch train loss 0.9564138092551241\n",
      "260-th epoch val loss 1.001615188408846\n",
      "261-th epoch train loss 0.9558943172598499\n",
      "261-th epoch val loss 1.0009618249885124\n",
      "262-th epoch train loss 0.955399803958325\n",
      "262-th epoch val loss 1.0003354763313173\n",
      "263-th epoch train loss 0.9549293538500315\n",
      "263-th epoch val loss 0.9997351969771935\n",
      "264-th epoch train loss 0.9544820820906278\n",
      "264-th epoch val loss 0.9991600725307674\n",
      "265-th epoch train loss 0.9540571335039927\n",
      "265-th epoch val loss 0.9986092186689022\n",
      "266-th epoch train loss 0.9536536816255428\n",
      "266-th epoch val loss 0.9980817801795312\n",
      "267-th epoch train loss 0.9532709277758512\n",
      "267-th epoch val loss 0.9975769300307986\n",
      "268-th epoch train loss 0.9529081001636035\n",
      "268-th epoch val loss 0.9970938684695574\n",
      "269-th epoch train loss 0.9525644530169796\n",
      "269-th epoch val loss 0.996631822148307\n",
      "270-th epoch train loss 0.9522392657425652\n",
      "270-th epoch val loss 0.996190043279678\n",
      "271-th epoch train loss 0.9519318421109321\n",
      "271-th epoch val loss 0.9957678088176041\n",
      "272-th epoch train loss 0.9516415094680394\n",
      "272-th epoch val loss 0.9953644196643359\n",
      "273-th epoch train loss 0.9513676179716599\n",
      "273-th epoch val loss 0.9949791999025\n",
      "274-th epoch train loss 0.9511095398520313\n",
      "274-th epoch val loss 0.994611496051407\n",
      "275-th epoch train loss 0.9508666686959804\n",
      "275-th epoch val loss 0.9942606763468533\n",
      "276-th epoch train loss 0.9506384187537782\n",
      "276-th epoch val loss 0.9939261300436801\n",
      "277-th epoch train loss 0.950424224268012\n",
      "277-th epoch val loss 0.9936072667403765\n",
      "278-th epoch train loss 0.9502235388237831\n",
      "278-th epoch val loss 0.9933035157250291\n",
      "279-th epoch train loss 0.9500358347195588\n",
      "279-th epoch val loss 0.9930143253419609\n",
      "280-th epoch train loss 0.9498606023580317\n",
      "280-th epoch val loss 0.9927391623783969\n",
      "281-th epoch train loss 0.9496973496563517\n",
      "281-th epoch val loss 0.9924775114705378\n",
      "282-th epoch train loss 0.9495456014751268\n",
      "282-th epoch val loss 0.9922288745284302\n",
      "283-th epoch train loss 0.9494048990655983\n",
      "283-th epoch val loss 0.9919927701790385\n",
      "284-th epoch train loss 0.9492747995344235\n",
      "284-th epoch val loss 0.9917687332269579\n",
      "285-th epoch train loss 0.9491548753255009\n",
      "285-th epoch val loss 0.991556314132203\n",
      "286-th epoch train loss 0.9490447137183183\n",
      "286-th epoch val loss 0.9913550785045446\n",
      "287-th epoch train loss 0.9489439163422857\n",
      "287-th epoch val loss 0.9911646066138698\n",
      "288-th epoch train loss 0.9488520987065621\n",
      "288-th epoch val loss 0.9909844929160662\n",
      "289-th epoch train loss 0.948768889744881\n",
      "289-th epoch val loss 0.9908143455939379\n",
      "290-th epoch train loss 0.9486939313749106\n",
      "290-th epoch val loss 0.9906537861126906\n",
      "291-th epoch train loss 0.9486268780716764\n",
      "291-th epoch val loss 0.99050244878951\n",
      "292-th epoch train loss 0.9485673964546201\n",
      "292-th epoch val loss 0.9903599803768166\n",
      "293-th epoch train loss 0.9485151648878493\n",
      "293-th epoch val loss 0.9902260396587397\n",
      "294-th epoch train loss 0.9484698730931732\n",
      "294-th epoch val loss 0.990100297060415\n",
      "295-th epoch train loss 0.9484312217755184\n",
      "295-th epoch val loss 0.9899824342696961\n",
      "296-th epoch train loss 0.9483989222603271\n",
      "296-th epoch val loss 0.9898721438708853\n",
      "297-th epoch train loss 0.9483726961425705\n",
      "297-th epoch val loss 0.9897691289901125\n",
      "298-th epoch train loss 0.9483522749470044\n",
      "298-th epoch val loss 0.9896731029519933\n",
      "299-th epoch train loss 0.9483373997993129\n",
      "299-th epoch val loss 0.9895837889472099\n",
      "300-th epoch train loss 0.9483278211077959\n",
      "300-th epoch val loss 0.9895009197106726\n",
      "301-th epoch train loss 0.9483232982552708\n",
      "301-th epoch val loss 0.989424237209929\n",
      "302-th epoch train loss 0.9483235993008632\n",
      "302-th epoch val loss 0.9893534923434986\n",
      "303-th epoch train loss 0.9483285006913721\n",
      "303-th epoch val loss 0.9892884446488218\n",
      "304-th epoch train loss 0.948337786981911\n",
      "304-th epoch val loss 0.9892288620195121\n",
      "305-th epoch train loss 0.9483512505655299\n",
      "305-th epoch val loss 0.9891745204316338\n",
      "306-th epoch train loss 0.9483686914115338\n",
      "306-th epoch val loss 0.9891252036787062\n",
      "307-th epoch train loss 0.9483899168122231\n",
      "307-th epoch val loss 0.9890807031151679\n",
      "308-th epoch train loss 0.9484147411377899\n",
      "308-th epoch val loss 0.9890408174080308\n",
      "309-th epoch train loss 0.9484429855991109\n",
      "309-th epoch val loss 0.989005352296469\n",
      "310-th epoch train loss 0.9484744780181881\n",
      "310-th epoch val loss 0.9889741203590893\n",
      "311-th epoch train loss 0.9485090526059953\n",
      "311-th epoch val loss 0.9889469407886408\n",
      "312-th epoch train loss 0.9485465497474939\n",
      "312-th epoch val loss 0.9889236391739344\n",
      "313-th epoch train loss 0.9485868157935925\n",
      "313-th epoch val loss 0.9889040472887365\n",
      "314-th epoch train loss 0.9486297028598316\n",
      "314-th epoch val loss 0.9888880028874251\n",
      "315-th epoch train loss 0.9486750686315771\n",
      "315-th epoch val loss 0.9888753495071881\n",
      "316-th epoch train loss 0.948722776175517\n",
      "316-th epoch val loss 0.9888659362765617\n",
      "317-th epoch train loss 0.9487726937572639\n",
      "317-th epoch val loss 0.9888596177301079\n",
      "318-th epoch train loss 0.948824694664866\n",
      "318-th epoch val loss 0.9888562536290327\n",
      "319-th epoch train loss 0.9488786570380422\n",
      "319-th epoch val loss 0.9888557087875692\n",
      "320-th epoch train loss 0.9489344637029588\n",
      "320-th epoch val loss 0.9888578529049258\n",
      "321-th epoch train loss 0.9489920020123652\n",
      "321-th epoch val loss 0.9888625604026409\n",
      "322-th epoch train loss 0.9490511636909342\n",
      "322-th epoch val loss 0.9888697102671612\n",
      "323-th epoch train loss 0.9491118446856234\n",
      "323-th epoch val loss 0.988879185897487\n",
      "324-th epoch train loss 0.9491739450209087\n",
      "324-th epoch val loss 0.9888908749577127\n",
      "325-th epoch train loss 0.9492373686587344\n",
      "325-th epoch val loss 0.9889046692343264\n",
      "326-th epoch train loss 0.9493020233630246\n",
      "326-th epoch val loss 0.9889204644980953\n",
      "327-th epoch train loss 0.9493678205686173\n",
      "327-th epoch val loss 0.9889381603704107\n",
      "328-th epoch train loss 0.9494346752544733\n",
      "328-th epoch val loss 0.9889576601939392\n",
      "329-th epoch train loss 0.949502505821031\n",
      "329-th epoch val loss 0.9889788709074516\n",
      "330-th epoch train loss 0.9495712339715714\n",
      "330-th epoch val loss 0.9890017029246935\n",
      "331-th epoch train loss 0.9496407845974609\n",
      "331-th epoch val loss 0.9890260700171668\n",
      "332-th epoch train loss 0.9497110856671601\n",
      "332-th epoch val loss 0.989051889200705\n",
      "333-th epoch train loss 0.9497820681188648\n",
      "333-th epoch val loss 0.9890790806257157\n",
      "334-th epoch train loss 0.9498536657566761\n",
      "334-th epoch val loss 0.9891075674709793\n",
      "335-th epoch train loss 0.9499258151501716\n",
      "335-th epoch val loss 0.9891372758408848\n",
      "336-th epoch train loss 0.9499984555372898\n",
      "336-th epoch val loss 0.9891681346660002\n",
      "337-th epoch train loss 0.9500715287303996\n",
      "337-th epoch val loss 0.9892000756068657\n",
      "338-th epoch train loss 0.9501449790254689\n",
      "338-th epoch val loss 0.9892330329609138\n",
      "339-th epoch train loss 0.9502187531142275\n",
      "339-th epoch val loss 0.9892669435724101\n",
      "340-th epoch train loss 0.9502927999992277\n",
      "340-th epoch val loss 0.9893017467453235\n",
      "341-th epoch train loss 0.9503670709117105\n",
      "341-th epoch val loss 0.989337384159032\n",
      "342-th epoch train loss 0.9504415192321901\n",
      "342-th epoch val loss 0.9893737997867701\n",
      "343-th epoch train loss 0.9505161004136665\n",
      "343-th epoch val loss 0.9894109398167396\n",
      "344-th epoch train loss 0.9505907719073802\n",
      "344-th epoch val loss 0.9894487525757852\n",
      "345-th epoch train loss 0.950665493091036\n",
      "345-th epoch val loss 0.9894871884555683\n",
      "346-th epoch train loss 0.9507402251994042\n",
      "346-th epoch val loss 0.9895261998411478\n",
      "347-th epoch train loss 0.9508149312572369\n",
      "347-th epoch val loss 0.9895657410418985\n",
      "348-th epoch train loss 0.9508895760144087\n",
      "348-th epoch val loss 0.9896057682246867\n",
      "349-th epoch train loss 0.9509641258832289\n",
      "349-th epoch val loss 0.989646239349237\n",
      "350-th epoch train loss 0.9510385488778375\n",
      "350-th epoch val loss 0.9896871141056163\n",
      "351-th epoch train loss 0.9511128145556313\n",
      "351-th epoch val loss 0.9897283538537682\n",
      "352-th epoch train loss 0.9511868939606486\n",
      "352-th epoch val loss 0.989769921565035\n",
      "353-th epoch train loss 0.9512607595688479\n",
      "353-th epoch val loss 0.9898117817655976\n",
      "354-th epoch train loss 0.9513343852352257\n",
      "354-th epoch val loss 0.9898539004817812\n",
      "355-th epoch train loss 0.9514077461427074\n",
      "355-th epoch val loss 0.989896245187155\n",
      "356-th epoch train loss 0.9514808187527597\n",
      "356-th epoch val loss 0.9899387847513853\n",
      "357-th epoch train loss 0.951553580757664\n",
      "357-th epoch val loss 0.9899814893907658\n",
      "358-th epoch train loss 0.9516260110344015\n",
      "358-th epoch val loss 0.990024330620389\n",
      "359-th epoch train loss 0.9516980896000945\n",
      "359-th epoch val loss 0.9900672812078956\n",
      "360-th epoch train loss 0.9517697975689543\n",
      "360-th epoch val loss 0.9901103151287546\n",
      "361-th epoch train loss 0.951841117110692\n",
      "361-th epoch val loss 0.990153407523028\n",
      "362-th epoch train loss 0.9519120314103328\n",
      "362-th epoch val loss 0.9901965346535664\n",
      "363-th epoch train loss 0.9519825246294022\n",
      "363-th epoch val loss 0.990239673865591\n",
      "364-th epoch train loss 0.9520525818684328\n",
      "364-th epoch val loss 0.990282803547628\n",
      "365-th epoch train loss 0.9521221891307432\n",
      "365-th epoch val loss 0.9903259030937298\n",
      "366-th epoch train loss 0.9521913332874625\n",
      "366-th epoch val loss 0.9903689528669681\n",
      "367-th epoch train loss 0.9522600020437435\n",
      "367-th epoch val loss 0.9904119341641371\n",
      "368-th epoch train loss 0.9523281839061402\n",
      "368-th epoch val loss 0.990454829181636\n",
      "369-th epoch train loss 0.9523958681510989\n",
      "369-th epoch val loss 0.9904976209824983\n",
      "370-th epoch train loss 0.9524630447945368\n",
      "370-th epoch val loss 0.9905402934645164\n",
      "371-th epoch train loss 0.9525297045624688\n",
      "371-th epoch val loss 0.9905828313294442\n",
      "372-th epoch train loss 0.952595838862644\n",
      "372-th epoch val loss 0.990625220053226\n",
      "373-th epoch train loss 0.95266143975717\n",
      "373-th epoch val loss 0.9906674458572313\n",
      "374-th epoch train loss 0.9527264999360773\n",
      "374-th epoch val loss 0.9907094956804567\n",
      "375-th epoch train loss 0.9527910126918093\n",
      "375-th epoch val loss 0.990751357152662\n",
      "376-th epoch train loss 0.9528549718945941\n",
      "376-th epoch val loss 0.9907930185684196\n",
      "377-th epoch train loss 0.9529183719686797\n",
      "377-th epoch val loss 0.9908344688620352\n",
      "378-th epoch train loss 0.9529812078693944\n",
      "378-th epoch val loss 0.9908756975833213\n",
      "379-th epoch train loss 0.953043475061015\n",
      "379-th epoch val loss 0.9909166948741913\n",
      "380-th epoch train loss 0.9531051694954084\n",
      "380-th epoch val loss 0.9909574514460473\n",
      "381-th epoch train loss 0.953166287591426\n",
      "381-th epoch val loss 0.9909979585579407\n",
      "382-th epoch train loss 0.9532268262150242\n",
      "382-th epoch val loss 0.9910382079954734\n",
      "383-th epoch train loss 0.9532867826600872\n",
      "383-th epoch val loss 0.9910781920504207\n",
      "384-th epoch train loss 0.9533461546299298\n",
      "384-th epoch val loss 0.9911179035010519\n",
      "385-th epoch train loss 0.9534049402194541\n",
      "385-th epoch val loss 0.9911573355931214\n",
      "386-th epoch train loss 0.953463137897948\n",
      "386-th epoch val loss 0.9911964820215174\n",
      "387-th epoch train loss 0.9535207464924903\n",
      "387-th epoch val loss 0.9912353369125368\n",
      "388-th epoch train loss 0.9535777651719535\n",
      "388-th epoch val loss 0.9912738948067721\n",
      "389-th epoch train loss 0.9536341934315805\n",
      "389-th epoch val loss 0.9913121506425878\n",
      "390-th epoch train loss 0.9536900310781173\n",
      "390-th epoch val loss 0.9913500997401707\n",
      "391-th epoch train loss 0.9537452782154766\n",
      "391-th epoch val loss 0.9913877377861294\n",
      "392-th epoch train loss 0.9537999352309292\n",
      "392-th epoch val loss 0.9914250608186269\n",
      "393-th epoch train loss 0.9538540027817897\n",
      "393-th epoch val loss 0.9914620652130375\n",
      "394-th epoch train loss 0.9539074817825901\n",
      "394-th epoch val loss 0.9914987476680956\n",
      "395-th epoch train loss 0.9539603733927216\n",
      "395-th epoch val loss 0.9915351051925326\n",
      "396-th epoch train loss 0.9540126790045258\n",
      "396-th epoch val loss 0.9915711350921766\n",
      "397-th epoch train loss 0.9540644002318317\n",
      "397-th epoch val loss 0.9916068349575126\n",
      "398-th epoch train loss 0.9541155388989058\n",
      "398-th epoch val loss 0.9916422026516707\n",
      "399-th epoch train loss 0.95416609702982\n",
      "399-th epoch val loss 0.9916772362988463\n",
      "400-th epoch train loss 0.954216076838207\n",
      "400-th epoch val loss 0.9917119342731225\n",
      "401-th epoch train loss 0.9542654807174027\n",
      "401-th epoch val loss 0.9917462951876896\n",
      "402-th epoch train loss 0.954314311230956\n",
      "402-th epoch val loss 0.9917803178844511\n",
      "403-th epoch train loss 0.9543625711034942\n",
      "403-th epoch val loss 0.991814001423995\n",
      "404-th epoch train loss 0.9544102632119338\n",
      "404-th epoch val loss 0.9918473450759238\n",
      "405-th epoch train loss 0.9544573905770232\n",
      "405-th epoch val loss 0.9918803483095329\n",
      "406-th epoch train loss 0.9545039563552074\n",
      "406-th epoch val loss 0.9919130107848185\n",
      "407-th epoch train loss 0.9545499638308045\n",
      "407-th epoch val loss 0.9919453323438151\n",
      "408-th epoch train loss 0.9545954164084787\n",
      "408-th epoch val loss 0.9919773130022396\n",
      "409-th epoch train loss 0.954640317606007\n",
      "409-th epoch val loss 0.9920089529414399\n",
      "410-th epoch train loss 0.9546846710473242\n",
      "410-th epoch val loss 0.9920402525006381\n",
      "411-th epoch train loss 0.9547284804558354\n",
      "411-th epoch val loss 0.9920712121694497\n",
      "412-th epoch train loss 0.9547717496479958\n",
      "412-th epoch val loss 0.9921018325806831\n",
      "413-th epoch train loss 0.9548144825271371\n",
      "413-th epoch val loss 0.9921321145033966\n",
      "414-th epoch train loss 0.9548566830775383\n",
      "414-th epoch val loss 0.9921620588362158\n",
      "415-th epoch train loss 0.9548983553587337\n",
      "415-th epoch val loss 0.9921916666008901\n",
      "416-th epoch train loss 0.9549395035000473\n",
      "416-th epoch val loss 0.992220938936097\n",
      "417-th epoch train loss 0.9549801316953439\n",
      "417-th epoch val loss 0.9922498770914677\n",
      "418-th epoch train loss 0.9550202441979947\n",
      "418-th epoch val loss 0.9922784824218402\n",
      "419-th epoch train loss 0.9550598453160448\n",
      "419-th epoch val loss 0.992306756381725\n",
      "420-th epoch train loss 0.95509893940758\n",
      "420-th epoch val loss 0.9923347005199807\n",
      "421-th epoch train loss 0.9551375308762814\n",
      "421-th epoch val loss 0.992362316474687\n",
      "422-th epoch train loss 0.9551756241671647\n",
      "422-th epoch val loss 0.9923896059682128\n",
      "423-th epoch train loss 0.9552132237624983\n",
      "423-th epoch val loss 0.9924165708024754\n",
      "424-th epoch train loss 0.9552503341778888\n",
      "424-th epoch val loss 0.9924432128543732\n",
      "425-th epoch train loss 0.9552869599585351\n",
      "425-th epoch val loss 0.9924695340713995\n",
      "426-th epoch train loss 0.9553231056756376\n",
      "426-th epoch val loss 0.9924955364674216\n",
      "427-th epoch train loss 0.955358775922966\n",
      "427-th epoch val loss 0.9925212221186249\n",
      "428-th epoch train loss 0.9553939753135706\n",
      "428-th epoch val loss 0.9925465931596139\n",
      "429-th epoch train loss 0.9554287084766385\n",
      "429-th epoch val loss 0.9925716517796629\n",
      "430-th epoch train loss 0.9554629800544867\n",
      "430-th epoch val loss 0.9925964002191181\n",
      "431-th epoch train loss 0.9554967946996898\n",
      "431-th epoch val loss 0.9926208407659382\n",
      "432-th epoch train loss 0.9555301570723344\n",
      "432-th epoch val loss 0.9926449757523734\n",
      "433-th epoch train loss 0.955563071837395\n",
      "433-th epoch val loss 0.9926688075517736\n",
      "434-th epoch train loss 0.9555955436622332\n",
      "434-th epoch val loss 0.9926923385755299\n",
      "435-th epoch train loss 0.9556275772142098\n",
      "435-th epoch val loss 0.9927155712701349\n",
      "436-th epoch train loss 0.9556591771584049\n",
      "436-th epoch val loss 0.9927385081143614\n",
      "437-th epoch train loss 0.9556903481554495\n",
      "437-th epoch val loss 0.9927611516165616\n",
      "438-th epoch train loss 0.9557210948594553\n",
      "438-th epoch val loss 0.9927835043120694\n",
      "439-th epoch train loss 0.9557514219160461\n",
      "439-th epoch val loss 0.9928055687607158\n",
      "440-th epoch train loss 0.9557813339604841\n",
      "440-th epoch val loss 0.9928273475444438\n",
      "441-th epoch train loss 0.9558108356158882\n",
      "441-th epoch val loss 0.9928488432650262\n",
      "442-th epoch train loss 0.9558399314915379\n",
      "442-th epoch val loss 0.9928700585418748\n",
      "443-th epoch train loss 0.9558686261812677\n",
      "443-th epoch val loss 0.992890996009948\n",
      "444-th epoch train loss 0.9558969242619388\n",
      "444-th epoch val loss 0.9929116583177434\n",
      "445-th epoch train loss 0.9559248302919935\n",
      "445-th epoch val loss 0.9929320481253788\n",
      "446-th epoch train loss 0.9559523488100822\n",
      "446-th epoch val loss 0.9929521681027557\n",
      "447-th epoch train loss 0.955979484333765\n",
      "447-th epoch val loss 0.9929720209278031\n",
      "448-th epoch train loss 0.9560062413582842\n",
      "448-th epoch val loss 0.992991609284799\n",
      "449-th epoch train loss 0.9560326243554053\n",
      "449-th epoch val loss 0.9930109358627672\n",
      "450-th epoch train loss 0.9560586377723204\n",
      "450-th epoch val loss 0.9930300033539442\n",
      "451-th epoch train loss 0.9560842860306169\n",
      "451-th epoch val loss 0.993048814452317\n",
      "452-th epoch train loss 0.9561095735253058\n",
      "452-th epoch val loss 0.9930673718522273\n",
      "453-th epoch train loss 0.9561345046239074\n",
      "453-th epoch val loss 0.9930856782470414\n",
      "454-th epoch train loss 0.9561590836655928\n",
      "454-th epoch val loss 0.9931037363278785\n",
      "455-th epoch train loss 0.9561833149603791\n",
      "455-th epoch val loss 0.9931215487824033\n",
      "456-th epoch train loss 0.9562072027883782\n",
      "456-th epoch val loss 0.9931391182936752\n",
      "457-th epoch train loss 0.9562307513990902\n",
      "457-th epoch val loss 0.9931564475390486\n",
      "458-th epoch train loss 0.9562539650107492\n",
      "458-th epoch val loss 0.9931735391891359\n",
      "459-th epoch train loss 0.9562768478097139\n",
      "459-th epoch val loss 0.9931903959068119\n",
      "460-th epoch train loss 0.9562994039498969\n",
      "460-th epoch val loss 0.993207020346275\n",
      "461-th epoch train loss 0.9563216375522452\n",
      "461-th epoch val loss 0.9932234151521525\n",
      "462-th epoch train loss 0.9563435527042515\n",
      "462-th epoch val loss 0.9932395829586556\n",
      "463-th epoch train loss 0.9563651534595123\n",
      "463-th epoch val loss 0.993255526388772\n",
      "464-th epoch train loss 0.9563864438373175\n",
      "464-th epoch val loss 0.9932712480535111\n",
      "465-th epoch train loss 0.9564074278222773\n",
      "465-th epoch val loss 0.9932867505511795\n",
      "466-th epoch train loss 0.9564281093639876\n",
      "466-th epoch val loss 0.9933020364667055\n",
      "467-th epoch train loss 0.9564484923767174\n",
      "467-th epoch val loss 0.9933171083709932\n",
      "468-th epoch train loss 0.9564685807391434\n",
      "468-th epoch val loss 0.9933319688203227\n",
      "469-th epoch train loss 0.9564883782940975\n",
      "469-th epoch val loss 0.9933466203557735\n",
      "470-th epoch train loss 0.9565078888483568\n",
      "470-th epoch val loss 0.9933610655026934\n",
      "471-th epoch train loss 0.9565271161724525\n",
      "471-th epoch val loss 0.9933753067701928\n",
      "472-th epoch train loss 0.9565460640005113\n",
      "472-th epoch val loss 0.9933893466506714\n",
      "473-th epoch train loss 0.9565647360301168\n",
      "473-th epoch val loss 0.9934031876193785\n",
      "474-th epoch train loss 0.9565831359221988\n",
      "474-th epoch val loss 0.9934168321339962\n",
      "475-th epoch train loss 0.9566012673009437\n",
      "475-th epoch val loss 0.9934302826342567\n",
      "476-th epoch train loss 0.9566191337537276\n",
      "476-th epoch val loss 0.9934435415415798\n",
      "477-th epoch train loss 0.9566367388310711\n",
      "477-th epoch val loss 0.9934566112587414\n",
      "478-th epoch train loss 0.956654086046613\n",
      "478-th epoch val loss 0.9934694941695655\n",
      "479-th epoch train loss 0.9566711788771024\n",
      "479-th epoch val loss 0.993482192638634\n",
      "480-th epoch train loss 0.9566880207624143\n",
      "480-th epoch val loss 0.9934947090110311\n",
      "481-th epoch train loss 0.9567046151055731\n",
      "481-th epoch val loss 0.9935070456120931\n",
      "482-th epoch train loss 0.956720965272805\n",
      "482-th epoch val loss 0.993519204747197\n",
      "483-th epoch train loss 0.9567370745935929\n",
      "483-th epoch val loss 0.9935311887015532\n",
      "484-th epoch train loss 0.9567529463607591\n",
      "484-th epoch val loss 0.9935429997400281\n",
      "485-th epoch train loss 0.9567685838305522\n",
      "485-th epoch val loss 0.9935546401069786\n",
      "486-th epoch train loss 0.9567839902227537\n",
      "486-th epoch val loss 0.9935661120261089\n",
      "487-th epoch train loss 0.9567991687207958\n",
      "487-th epoch val loss 0.9935774177003404\n",
      "488-th epoch train loss 0.9568141224718899\n",
      "488-th epoch val loss 0.9935885593116989\n",
      "489-th epoch train loss 0.9568288545871696\n",
      "489-th epoch val loss 0.993599539021219\n",
      "490-th epoch train loss 0.9568433681418426\n",
      "490-th epoch val loss 0.99361035896886\n",
      "491-th epoch train loss 0.9568576661753537\n",
      "491-th epoch val loss 0.9936210212734388\n",
      "492-th epoch train loss 0.9568717516915595\n",
      "492-th epoch val loss 0.9936315280325754\n",
      "493-th epoch train loss 0.9568856276589073\n",
      "493-th epoch val loss 0.9936418813226484\n",
      "494-th epoch train loss 0.9568992970106303\n",
      "494-th epoch val loss 0.9936520831987691\n",
      "495-th epoch train loss 0.9569127626449438\n",
      "495-th epoch val loss 0.9936621356947605\n",
      "496-th epoch train loss 0.9569260274252548\n",
      "496-th epoch val loss 0.9936720408231524\n",
      "497-th epoch train loss 0.9569390941803758\n",
      "497-th epoch val loss 0.9936818005751861\n",
      "498-th epoch train loss 0.9569519657047457\n",
      "498-th epoch val loss 0.9936914169208255\n",
      "499-th epoch train loss 0.9569646447586605\n",
      "499-th epoch val loss 0.9937008918087842\n",
      "500-th epoch train loss 0.9569771340685047\n",
      "500-th epoch val loss 0.9937102271665573\n",
      "501-th epoch train loss 0.9569894363269928\n",
      "501-th epoch val loss 0.9937194249004639\n",
      "502-th epoch train loss 0.957001554193414\n",
      "502-th epoch val loss 0.9937284868956973\n",
      "503-th epoch train loss 0.9570134902938843\n",
      "503-th epoch val loss 0.9937374150163839\n",
      "504-th epoch train loss 0.9570252472215988\n",
      "504-th epoch val loss 0.9937462111056491\n",
      "505-th epoch train loss 0.957036827537091\n",
      "505-th epoch val loss 0.9937548769856875\n",
      "506-th epoch train loss 0.9570482337684983\n",
      "506-th epoch val loss 0.9937634144578497\n",
      "507-th epoch train loss 0.9570594684118271\n",
      "507-th epoch val loss 0.9937718253027213\n",
      "508-th epoch train loss 0.9570705339312214\n",
      "508-th epoch val loss 0.9937801112802206\n",
      "509-th epoch train loss 0.9570814327592385\n",
      "509-th epoch val loss 0.9937882741296942\n",
      "510-th epoch train loss 0.9570921672971237\n",
      "510-th epoch val loss 0.9937963155700227\n",
      "511-th epoch train loss 0.9571027399150874\n",
      "511-th epoch val loss 0.9938042372997287\n",
      "512-th epoch train loss 0.9571131529525884\n",
      "512-th epoch val loss 0.9938120409970904\n",
      "513-th epoch train loss 0.9571234087186132\n",
      "513-th epoch val loss 0.9938197283202617\n",
      "514-th epoch train loss 0.9571335094919632\n",
      "514-th epoch val loss 0.9938273009073927\n",
      "515-th epoch train loss 0.95714345752154\n",
      "515-th epoch val loss 0.9938347603767603\n",
      "516-th epoch train loss 0.9571532550266306\n",
      "516-th epoch val loss 0.9938421083268938\n",
      "517-th epoch train loss 0.9571629041971985\n",
      "517-th epoch val loss 0.9938493463367148\n",
      "518-th epoch train loss 0.9571724071941733\n",
      "518-th epoch val loss 0.9938564759656714\n",
      "519-th epoch train loss 0.9571817661497366\n",
      "519-th epoch val loss 0.9938634987538784\n",
      "520-th epoch train loss 0.9571909831676187\n",
      "520-th epoch val loss 0.9938704162222641\n",
      "521-th epoch train loss 0.9572000603233846\n",
      "521-th epoch val loss 0.9938772298727142\n",
      "522-th epoch train loss 0.9572089996647287\n",
      "522-th epoch val loss 0.9938839411882227\n",
      "523-th epoch train loss 0.9572178032117656\n",
      "523-th epoch val loss 0.9938905516330411\n",
      "524-th epoch train loss 0.957226472957321\n",
      "524-th epoch val loss 0.9938970626528337\n",
      "525-th epoch train loss 0.9572350108672221\n",
      "525-th epoch val loss 0.9939034756748304\n",
      "526-th epoch train loss 0.9572434188805907\n",
      "526-th epoch val loss 0.9939097921079877\n",
      "527-th epoch train loss 0.9572516989101334\n",
      "527-th epoch val loss 0.9939160133431441\n",
      "528-th epoch train loss 0.9572598528424302\n",
      "528-th epoch val loss 0.9939221407531821\n",
      "529-th epoch train loss 0.9572678825382246\n",
      "529-th epoch val loss 0.9939281756931897\n",
      "530-th epoch train loss 0.9572757898327118\n",
      "530-th epoch val loss 0.9939341195006217\n",
      "531-th epoch train loss 0.9572835765358274\n",
      "531-th epoch val loss 0.9939399734954667\n",
      "532-th epoch train loss 0.9572912444325321\n",
      "532-th epoch val loss 0.9939457389804085\n",
      "533-th epoch train loss 0.9572987952830997\n",
      "533-th epoch val loss 0.9939514172409937\n",
      "534-th epoch train loss 0.9573062308233989\n",
      "534-th epoch val loss 0.9939570095457989\n",
      "535-th epoch train loss 0.9573135527651775\n",
      "535-th epoch val loss 0.9939625171465944\n",
      "536-th epoch train loss 0.9573207627963463\n",
      "536-th epoch val loss 0.9939679412785156\n",
      "537-th epoch train loss 0.9573278625812544\n",
      "537-th epoch val loss 0.9939732831602264\n",
      "538-th epoch train loss 0.9573348537609734\n",
      "538-th epoch val loss 0.9939785439940919\n",
      "539-th epoch train loss 0.957341737953571\n",
      "539-th epoch val loss 0.9939837249663425\n",
      "540-th epoch train loss 0.9573485167543891\n",
      "540-th epoch val loss 0.9939888272472447\n",
      "541-th epoch train loss 0.9573551917363156\n",
      "541-th epoch val loss 0.9939938519912681\n",
      "542-th epoch train loss 0.9573617644500589\n",
      "542-th epoch val loss 0.9939988003372553\n",
      "543-th epoch train loss 0.9573682364244167\n",
      "543-th epoch val loss 0.9940036734085864\n",
      "544-th epoch train loss 0.9573746091665452\n",
      "544-th epoch val loss 0.9940084723133514\n",
      "545-th epoch train loss 0.9573808841622263\n",
      "545-th epoch val loss 0.9940131981445143\n",
      "546-th epoch train loss 0.9573870628761317\n",
      "546-th epoch val loss 0.9940178519800802\n",
      "547-th epoch train loss 0.9573931467520871\n",
      "547-th epoch val loss 0.9940224348832657\n",
      "548-th epoch train loss 0.9573991372133306\n",
      "548-th epoch val loss 0.99402694790266\n",
      "549-th epoch train loss 0.9574050356627747\n",
      "549-th epoch val loss 0.9940313920723942\n",
      "550-th epoch train loss 0.9574108434832591\n",
      "550-th epoch val loss 0.9940357684123031\n",
      "551-th epoch train loss 0.9574165620378119\n",
      "551-th epoch val loss 0.994040077928095\n",
      "552-th epoch train loss 0.9574221926698938\n",
      "552-th epoch val loss 0.9940443216115085\n",
      "553-th epoch train loss 0.9574277367036558\n",
      "553-th epoch val loss 0.9940485004404794\n",
      "554-th epoch train loss 0.9574331954441817\n",
      "554-th epoch val loss 0.9940526153793026\n",
      "555-th epoch train loss 0.9574385701777391\n",
      "555-th epoch val loss 0.9940566673787923\n",
      "556-th epoch train loss 0.9574438621720196\n",
      "556-th epoch val loss 0.9940606573764427\n",
      "557-th epoch train loss 0.9574490726763822\n",
      "557-th epoch val loss 0.9940645862965864\n",
      "558-th epoch train loss 0.9574542029220923\n",
      "558-th epoch val loss 0.9940684550505552\n",
      "559-th epoch train loss 0.9574592541225583\n",
      "559-th epoch val loss 0.9940722645368353\n",
      "560-th epoch train loss 0.9574642274735672\n",
      "560-th epoch val loss 0.9940760156412247\n",
      "561-th epoch train loss 0.9574691241535171\n",
      "561-th epoch val loss 0.9940797092369884\n",
      "562-th epoch train loss 0.9574739453236465\n",
      "562-th epoch val loss 0.994083346185012\n",
      "563-th epoch train loss 0.9574786921282635\n",
      "563-th epoch val loss 0.9940869273339571\n",
      "564-th epoch train loss 0.9574833656949701\n",
      "564-th epoch val loss 0.9940904535204094\n",
      "565-th epoch train loss 0.9574879671348873\n",
      "565-th epoch val loss 0.9940939255690338\n",
      "566-th epoch train loss 0.9574924975428757\n",
      "566-th epoch val loss 0.9940973442927223\n",
      "567-th epoch train loss 0.9574969579977526\n",
      "567-th epoch val loss 0.9941007104927414\n",
      "568-th epoch train loss 0.9575013495625112\n",
      "568-th epoch val loss 0.9941040249588817\n",
      "569-th epoch train loss 0.9575056732845321\n",
      "569-th epoch val loss 0.9941072884696015\n",
      "570-th epoch train loss 0.9575099301957974\n",
      "570-th epoch val loss 0.9941105017921752\n",
      "571-th epoch train loss 0.9575141213130993\n",
      "571-th epoch val loss 0.9941136656828322\n",
      "572-th epoch train loss 0.9575182476382459\n",
      "572-th epoch val loss 0.9941167808869039\n",
      "573-th epoch train loss 0.9575223101582705\n",
      "573-th epoch val loss 0.9941198481389623\n",
      "574-th epoch train loss 0.9575263098456286\n",
      "574-th epoch val loss 0.9941228681629593\n",
      "575-th epoch train loss 0.957530247658403\n",
      "575-th epoch val loss 0.9941258416723662\n",
      "576-th epoch train loss 0.9575341245404998\n",
      "576-th epoch val loss 0.9941287693703125\n",
      "577-th epoch train loss 0.9575379414218449\n",
      "577-th epoch val loss 0.9941316519497183\n",
      "578-th epoch train loss 0.9575416992185769\n",
      "578-th epoch val loss 0.9941344900934315\n",
      "579-th epoch train loss 0.9575453988332407\n",
      "579-th epoch val loss 0.9941372844743612\n",
      "580-th epoch train loss 0.9575490411549726\n",
      "580-th epoch val loss 0.9941400357556065\n",
      "581-th epoch train loss 0.9575526270596919\n",
      "581-th epoch val loss 0.9941427445905917\n",
      "582-th epoch train loss 0.9575561574102824\n",
      "582-th epoch val loss 0.9941454116231917\n",
      "583-th epoch train loss 0.9575596330567772\n",
      "583-th epoch val loss 0.9941480374878628\n",
      "584-th epoch train loss 0.9575630548365366\n",
      "584-th epoch val loss 0.9941506228097674\n",
      "585-th epoch train loss 0.9575664235744287\n",
      "585-th epoch val loss 0.9941531682048996\n",
      "586-th epoch train loss 0.9575697400830037\n",
      "586-th epoch val loss 0.9941556742802103\n",
      "587-th epoch train loss 0.9575730051626695\n",
      "587-th epoch val loss 0.9941581416337297\n",
      "588-th epoch train loss 0.9575762196018618\n",
      "588-th epoch val loss 0.9941605708546862\n",
      "589-th epoch train loss 0.9575793841772142\n",
      "589-th epoch val loss 0.9941629625236297\n",
      "590-th epoch train loss 0.9575824996537271\n",
      "590-th epoch val loss 0.9941653172125486\n",
      "591-th epoch train loss 0.9575855667849326\n",
      "591-th epoch val loss 0.9941676354849882\n",
      "592-th epoch train loss 0.9575885863130545\n",
      "592-th epoch val loss 0.9941699178961636\n",
      "593-th epoch train loss 0.9575915589691764\n",
      "593-th epoch val loss 0.9941721649930793\n",
      "594-th epoch train loss 0.9575944854733953\n",
      "594-th epoch val loss 0.9941743773146392\n",
      "595-th epoch train loss 0.9575973665349807\n",
      "595-th epoch val loss 0.994176555391759\n",
      "596-th epoch train loss 0.9576002028525313\n",
      "596-th epoch val loss 0.9941786997474787\n",
      "597-th epoch train loss 0.9576029951141265\n",
      "597-th epoch val loss 0.9941808108970708\n",
      "598-th epoch train loss 0.9576057439974781\n",
      "598-th epoch val loss 0.9941828893481487\n",
      "599-th epoch train loss 0.9576084501700809\n",
      "599-th epoch val loss 0.9941849356007747\n",
      "600-th epoch train loss 0.9576111142893579\n",
      "600-th epoch val loss 0.9941869501475628\n",
      "601-th epoch train loss 0.9576137370028089\n",
      "601-th epoch val loss 0.9941889334737878\n",
      "602-th epoch train loss 0.9576163189481517\n",
      "602-th epoch val loss 0.9941908860574846\n",
      "603-th epoch train loss 0.9576188607534645\n",
      "603-th epoch val loss 0.9941928083695512\n",
      "604-th epoch train loss 0.9576213630373265\n",
      "604-th epoch val loss 0.9941947008738516\n",
      "605-th epoch train loss 0.9576238264089553\n",
      "605-th epoch val loss 0.9941965640273113\n",
      "606-th epoch train loss 0.9576262514683432\n",
      "606-th epoch val loss 0.9941983982800175\n",
      "607-th epoch train loss 0.9576286388063926\n",
      "607-th epoch val loss 0.9942002040753194\n",
      "608-th epoch train loss 0.9576309890050464\n",
      "608-th epoch val loss 0.9942019818499171\n",
      "609-th epoch train loss 0.9576333026374212\n",
      "609-th epoch val loss 0.9942037320339628\n",
      "610-th epoch train loss 0.9576355802679358\n",
      "610-th epoch val loss 0.9942054550511494\n",
      "611-th epoch train loss 0.9576378224524383\n",
      "611-th epoch val loss 0.9942071513188077\n",
      "612-th epoch train loss 0.9576400297383316\n",
      "612-th epoch val loss 0.9942088212479931\n",
      "613-th epoch train loss 0.9576422026646968\n",
      "613-th epoch val loss 0.9942104652435756\n",
      "614-th epoch train loss 0.9576443417624182\n",
      "614-th epoch val loss 0.994212083704334\n",
      "615-th epoch train loss 0.9576464475543\n",
      "615-th epoch val loss 0.9942136770230366\n",
      "616-th epoch train loss 0.9576485205551881\n",
      "616-th epoch val loss 0.9942152455865325\n",
      "617-th epoch train loss 0.9576505612720876\n",
      "617-th epoch val loss 0.9942167897758352\n",
      "618-th epoch train loss 0.9576525702042764\n",
      "618-th epoch val loss 0.9942183099662067\n",
      "619-th epoch train loss 0.9576545478434205\n",
      "619-th epoch val loss 0.9942198065272417\n",
      "620-th epoch train loss 0.9576564946736873\n",
      "620-th epoch val loss 0.9942212798229486\n",
      "621-th epoch train loss 0.9576584111718559\n",
      "621-th epoch val loss 0.9942227302118324\n",
      "622-th epoch train loss 0.9576602978074269\n",
      "622-th epoch val loss 0.9942241580469716\n",
      "623-th epoch train loss 0.9576621550427293\n",
      "623-th epoch val loss 0.9942255636761014\n",
      "624-th epoch train loss 0.9576639833330295\n",
      "624-th epoch val loss 0.9942269474416869\n",
      "625-th epoch train loss 0.9576657831266332\n",
      "625-th epoch val loss 0.9942283096810038\n",
      "626-th epoch train loss 0.9576675548649891\n",
      "626-th epoch val loss 0.9942296507262108\n",
      "627-th epoch train loss 0.9576692989827964\n",
      "627-th epoch val loss 0.9942309709044284\n",
      "628-th epoch train loss 0.957671015908097\n",
      "628-th epoch val loss 0.994232270537808\n",
      "629-th epoch train loss 0.9576727060623814\n",
      "629-th epoch val loss 0.9942335499436102\n",
      "630-th epoch train loss 0.9576743698606841\n",
      "630-th epoch val loss 0.9942348094342719\n",
      "631-th epoch train loss 0.9576760077116787\n",
      "631-th epoch val loss 0.9942360493174779\n",
      "632-th epoch train loss 0.9576776200177761\n",
      "632-th epoch val loss 0.9942372698962337\n",
      "633-th epoch train loss 0.9576792071752152\n",
      "633-th epoch val loss 0.9942384714689321\n",
      "634-th epoch train loss 0.9576807695741572\n",
      "634-th epoch val loss 0.9942396543294207\n",
      "635-th epoch train loss 0.9576823075987762\n",
      "635-th epoch val loss 0.9942408187670708\n",
      "636-th epoch train loss 0.9576838216273493\n",
      "636-th epoch val loss 0.9942419650668424\n",
      "637-th epoch train loss 0.9576853120323439\n",
      "637-th epoch val loss 0.9942430935093486\n",
      "638-th epoch train loss 0.9576867791805069\n",
      "638-th epoch val loss 0.9942442043709222\n",
      "639-th epoch train loss 0.9576882234329482\n",
      "639-th epoch val loss 0.9942452979236773\n",
      "640-th epoch train loss 0.957689645145228\n",
      "640-th epoch val loss 0.9942463744355726\n",
      "641-th epoch train loss 0.9576910446674395\n",
      "641-th epoch val loss 0.9942474341704729\n",
      "642-th epoch train loss 0.9576924223442903\n",
      "642-th epoch val loss 0.9942484773882106\n",
      "643-th epoch train loss 0.9576937785151849\n",
      "643-th epoch val loss 0.9942495043446439\n",
      "644-th epoch train loss 0.957695113514303\n",
      "644-th epoch val loss 0.9942505152917175\n",
      "645-th epoch train loss 0.9576964276706814\n",
      "645-th epoch val loss 0.9942515104775212\n",
      "646-th epoch train loss 0.957697721308287\n",
      "646-th epoch val loss 0.9942524901463455\n",
      "647-th epoch train loss 0.9576989947460992\n",
      "647-th epoch val loss 0.9942534545387409\n",
      "648-th epoch train loss 0.9577002482981796\n",
      "648-th epoch val loss 0.9942544038915713\n",
      "649-th epoch train loss 0.9577014822737507\n",
      "649-th epoch val loss 0.9942553384380692\n",
      "650-th epoch train loss 0.9577026969772667\n",
      "650-th epoch val loss 0.9942562584078938\n",
      "651-th epoch train loss 0.9577038927084874\n",
      "651-th epoch val loss 0.99425716402718\n",
      "652-th epoch train loss 0.9577050697625475\n",
      "652-th epoch val loss 0.9942580555185928\n",
      "653-th epoch train loss 0.9577062284300281\n",
      "653-th epoch val loss 0.994258933101379\n",
      "654-th epoch train loss 0.9577073689970262\n",
      "654-th epoch val loss 0.9942597969914211\n",
      "655-th epoch train loss 0.957708491745221\n",
      "655-th epoch val loss 0.9942606474012837\n",
      "656-th epoch train loss 0.9577095969519424\n",
      "656-th epoch val loss 0.9942614845402651\n",
      "657-th epoch train loss 0.957710684890237\n",
      "657-th epoch val loss 0.9942623086144481\n",
      "658-th epoch train loss 0.9577117558289323\n",
      "658-th epoch val loss 0.9942631198267465\n",
      "659-th epoch train loss 0.9577128100327028\n",
      "659-th epoch val loss 0.9942639183769529\n",
      "660-th epoch train loss 0.9577138477621302\n",
      "660-th epoch val loss 0.994264704461786\n",
      "661-th epoch train loss 0.9577148692737691\n",
      "661-th epoch val loss 0.994265478274938\n",
      "662-th epoch train loss 0.9577158748202058\n",
      "662-th epoch val loss 0.9942662400071185\n",
      "663-th epoch train loss 0.9577168646501198\n",
      "663-th epoch val loss 0.9942669898461002\n",
      "664-th epoch train loss 0.9577178390083441\n",
      "664-th epoch val loss 0.9942677279767643\n",
      "665-th epoch train loss 0.9577187981359221\n",
      "665-th epoch val loss 0.9942684545811437\n",
      "666-th epoch train loss 0.9577197422701669\n",
      "666-th epoch val loss 0.9942691698384635\n",
      "667-th epoch train loss 0.9577206716447183\n",
      "667-th epoch val loss 0.9942698739251886\n",
      "668-th epoch train loss 0.957721586489597\n",
      "668-th epoch val loss 0.9942705670150606\n",
      "669-th epoch train loss 0.9577224870312633\n",
      "669-th epoch val loss 0.9942712492791427\n",
      "670-th epoch train loss 0.957723373492668\n",
      "670-th epoch val loss 0.9942719208858578\n",
      "671-th epoch train loss 0.9577242460933085\n",
      "671-th epoch val loss 0.9942725820010296\n",
      "672-th epoch train loss 0.9577251050492812\n",
      "672-th epoch val loss 0.9942732327879228\n",
      "673-th epoch train loss 0.9577259505733312\n",
      "673-th epoch val loss 0.9942738734072799\n",
      "674-th epoch train loss 0.9577267828749075\n",
      "674-th epoch val loss 0.9942745040173608\n",
      "675-th epoch train loss 0.9577276021602114\n",
      "675-th epoch val loss 0.9942751247739823\n",
      "676-th epoch train loss 0.9577284086322457\n",
      "676-th epoch val loss 0.994275735830552\n",
      "677-th epoch train loss 0.9577292024908652\n",
      "677-th epoch val loss 0.9942763373381062\n",
      "678-th epoch train loss 0.957729983932824\n",
      "678-th epoch val loss 0.9942769294453466\n",
      "679-th epoch train loss 0.9577307531518235\n",
      "679-th epoch val loss 0.9942775122986766\n",
      "680-th epoch train loss 0.9577315103385586\n",
      "680-th epoch val loss 0.9942780860422332\n",
      "681-th epoch train loss 0.9577322556807659\n",
      "681-th epoch val loss 0.9942786508179262\n",
      "682-th epoch train loss 0.9577329893632659\n",
      "682-th epoch val loss 0.994279206765467\n",
      "683-th epoch train loss 0.9577337115680097\n",
      "683-th epoch val loss 0.9942797540224061\n",
      "684-th epoch train loss 0.9577344224741249\n",
      "684-th epoch val loss 0.9942802927241646\n",
      "685-th epoch train loss 0.9577351222579543\n",
      "685-th epoch val loss 0.9942808230040661\n",
      "686-th epoch train loss 0.9577358110931037\n",
      "686-th epoch val loss 0.9942813449933716\n",
      "687-th epoch train loss 0.9577364891504805\n",
      "687-th epoch val loss 0.9942818588213067\n",
      "688-th epoch train loss 0.9577371565983366\n",
      "688-th epoch val loss 0.9942823646150954\n",
      "689-th epoch train loss 0.9577378136023103\n",
      "689-th epoch val loss 0.9942828624999911\n",
      "690-th epoch train loss 0.9577384603254641\n",
      "690-th epoch val loss 0.9942833525993052\n",
      "691-th epoch train loss 0.9577390969283266\n",
      "691-th epoch val loss 0.9942838350344392\n",
      "692-th epoch train loss 0.9577397235689294\n",
      "692-th epoch val loss 0.9942843099249096\n",
      "693-th epoch train loss 0.9577403404028488\n",
      "693-th epoch val loss 0.9942847773883817\n",
      "694-th epoch train loss 0.9577409475832401\n",
      "694-th epoch val loss 0.9942852375406955\n",
      "695-th epoch train loss 0.9577415452608776\n",
      "695-th epoch val loss 0.9942856904958934\n",
      "696-th epoch train loss 0.9577421335841891\n",
      "696-th epoch val loss 0.9942861363662492\n",
      "697-th epoch train loss 0.9577427126992941\n",
      "697-th epoch val loss 0.9942865752622936\n",
      "698-th epoch train loss 0.9577432827500381\n",
      "698-th epoch val loss 0.9942870072928421\n",
      "699-th epoch train loss 0.9577438438780289\n",
      "699-th epoch val loss 0.9942874325650221\n",
      "700-th epoch train loss 0.9577443962226694\n",
      "700-th epoch val loss 0.9942878511842957\n",
      "701-th epoch train loss 0.9577449399211934\n",
      "701-th epoch val loss 0.9942882632544892\n",
      "702-th epoch train loss 0.9577454751086982\n",
      "702-th epoch val loss 0.9942886688778159\n",
      "703-th epoch train loss 0.9577460019181773\n",
      "703-th epoch val loss 0.9942890681549008\n",
      "704-th epoch train loss 0.9577465204805553\n",
      "704-th epoch val loss 0.9942894611848069\n",
      "705-th epoch train loss 0.957747030924715\n",
      "705-th epoch val loss 0.9942898480650564\n",
      "706-th epoch train loss 0.9577475333775345\n",
      "706-th epoch val loss 0.9942902288916576\n",
      "707-th epoch train loss 0.9577480279639148\n",
      "707-th epoch val loss 0.9942906037591259\n",
      "708-th epoch train loss 0.9577485148068104\n",
      "708-th epoch val loss 0.9942909727605077\n",
      "709-th epoch train loss 0.9577489940272621\n",
      "709-th epoch val loss 0.9942913359874017\n",
      "710-th epoch train loss 0.9577494657444225\n",
      "710-th epoch val loss 0.9942916935299841\n",
      "711-th epoch train loss 0.9577499300755904\n",
      "711-th epoch val loss 0.9942920454770287\n",
      "712-th epoch train loss 0.957750387136234\n",
      "712-th epoch val loss 0.9942923919159263\n",
      "713-th epoch train loss 0.9577508370400236\n",
      "713-th epoch val loss 0.9942927329327125\n",
      "714-th epoch train loss 0.9577512798988559\n",
      "714-th epoch val loss 0.9942930686120796\n",
      "715-th epoch train loss 0.9577517158228863\n",
      "715-th epoch val loss 0.9942933990374061\n",
      "716-th epoch train loss 0.957752144920549\n",
      "716-th epoch val loss 0.9942937242907708\n",
      "717-th epoch train loss 0.957752567298591\n",
      "717-th epoch val loss 0.9942940444529758\n",
      "718-th epoch train loss 0.9577529830620924\n",
      "718-th epoch val loss 0.9942943596035658\n",
      "719-th epoch train loss 0.9577533923144971\n",
      "719-th epoch val loss 0.9942946698208477\n",
      "720-th epoch train loss 0.9577537951576328\n",
      "720-th epoch val loss 0.9942949751819085\n",
      "721-th epoch train loss 0.9577541916917415\n",
      "721-th epoch val loss 0.9942952757626342\n",
      "722-th epoch train loss 0.9577545820154996\n",
      "722-th epoch val loss 0.9942955716377304\n",
      "723-th epoch train loss 0.9577549662260465\n",
      "723-th epoch val loss 0.9942958628807399\n",
      "724-th epoch train loss 0.9577553444190037\n",
      "724-th epoch val loss 0.9942961495640583\n",
      "725-th epoch train loss 0.957755716688502\n",
      "725-th epoch val loss 0.9942964317589538\n",
      "726-th epoch train loss 0.957756083127204\n",
      "726-th epoch val loss 0.9942967095355856\n",
      "727-th epoch train loss 0.9577564438263246\n",
      "727-th epoch val loss 0.9942969829630186\n",
      "728-th epoch train loss 0.9577567988756567\n",
      "728-th epoch val loss 0.9942972521092419\n",
      "729-th epoch train loss 0.9577571483635918\n",
      "729-th epoch val loss 0.9942975170411852\n",
      "730-th epoch train loss 0.9577574923771408\n",
      "730-th epoch val loss 0.994297777824735\n",
      "731-th epoch train loss 0.9577578310019565\n",
      "731-th epoch val loss 0.9942980345247506\n",
      "732-th epoch train loss 0.9577581643223565\n",
      "732-th epoch val loss 0.9942982872050814\n",
      "733-th epoch train loss 0.9577584924213387\n",
      "733-th epoch val loss 0.9942985359285795\n",
      "734-th epoch train loss 0.9577588153806083\n",
      "734-th epoch val loss 0.9942987807571192\n",
      "735-th epoch train loss 0.9577591332805931\n",
      "735-th epoch val loss 0.994299021751608\n",
      "736-th epoch train loss 0.957759446200465\n",
      "736-th epoch val loss 0.9942992589720046\n",
      "737-th epoch train loss 0.9577597542181606\n",
      "737-th epoch val loss 0.9942994924773335\n",
      "738-th epoch train loss 0.9577600574103986\n",
      "738-th epoch val loss 0.9942997223256969\n",
      "739-th epoch train loss 0.9577603558526988\n",
      "739-th epoch val loss 0.9942999485742917\n",
      "740-th epoch train loss 0.9577606496194028\n",
      "740-th epoch val loss 0.9943001712794228\n",
      "741-th epoch train loss 0.9577609387836904\n",
      "741-th epoch val loss 0.9943003904965154\n",
      "742-th epoch train loss 0.9577612234175981\n",
      "742-th epoch val loss 0.994300606280132\n",
      "743-th epoch train loss 0.9577615035920375\n",
      "743-th epoch val loss 0.9943008186839821\n",
      "744-th epoch train loss 0.9577617793768122\n",
      "744-th epoch val loss 0.994301027760938\n",
      "745-th epoch train loss 0.9577620508406344\n",
      "745-th epoch val loss 0.9943012335630456\n",
      "746-th epoch train loss 0.9577623180511435\n",
      "746-th epoch val loss 0.9943014361415403\n",
      "747-th epoch train loss 0.9577625810749221\n",
      "747-th epoch val loss 0.9943016355468566\n",
      "748-th epoch train loss 0.9577628399775118\n",
      "748-th epoch val loss 0.9943018318286425\n",
      "749-th epoch train loss 0.9577630948234302\n",
      "749-th epoch val loss 0.99430202503577\n",
      "750-th epoch train loss 0.9577633456761865\n",
      "750-th epoch val loss 0.994302215216349\n",
      "751-th epoch train loss 0.9577635925982975\n",
      "751-th epoch val loss 0.9943024024177387\n",
      "752-th epoch train loss 0.957763835651302\n",
      "752-th epoch val loss 0.9943025866865567\n",
      "753-th epoch train loss 0.9577640748957786\n",
      "753-th epoch val loss 0.9943027680686948\n",
      "754-th epoch train loss 0.9577643103913578\n",
      "754-th epoch val loss 0.994302946609328\n",
      "755-th epoch train loss 0.9577645421967375\n",
      "755-th epoch val loss 0.9943031223529251\n",
      "756-th epoch train loss 0.957764770369699\n",
      "756-th epoch val loss 0.9943032953432606\n",
      "757-th epoch train loss 0.9577649949671202\n",
      "757-th epoch val loss 0.9943034656234248\n",
      "758-th epoch train loss 0.9577652160449885\n",
      "758-th epoch val loss 0.994303633235836\n",
      "759-th epoch train loss 0.9577654336584173\n",
      "759-th epoch val loss 0.9943037982222491\n",
      "760-th epoch train loss 0.9577656478616575\n",
      "760-th epoch val loss 0.9943039606237671\n",
      "761-th epoch train loss 0.9577658587081118\n",
      "761-th epoch val loss 0.9943041204808508\n",
      "762-th epoch train loss 0.9577660662503478\n",
      "762-th epoch val loss 0.9943042778333281\n",
      "763-th epoch train loss 0.9577662705401112\n",
      "763-th epoch val loss 0.9943044327204056\n",
      "764-th epoch train loss 0.9577664716283382\n",
      "764-th epoch val loss 0.9943045851806772\n",
      "765-th epoch train loss 0.9577666695651686\n",
      "765-th epoch val loss 0.9943047352521331\n",
      "766-th epoch train loss 0.9577668643999577\n",
      "766-th epoch val loss 0.9943048829721699\n",
      "767-th epoch train loss 0.9577670561812893\n",
      "767-th epoch val loss 0.9943050283776005\n",
      "768-th epoch train loss 0.9577672449569867\n",
      "768-th epoch val loss 0.9943051715046624\n",
      "769-th epoch train loss 0.9577674307741256\n",
      "769-th epoch val loss 0.994305312389025\n",
      "770-th epoch train loss 0.9577676136790452\n",
      "770-th epoch val loss 0.9943054510658026\n",
      "771-th epoch train loss 0.9577677937173593\n",
      "771-th epoch val loss 0.9943055875695587\n",
      "772-th epoch train loss 0.9577679709339686\n",
      "772-th epoch val loss 0.9943057219343173\n",
      "773-th epoch train loss 0.9577681453730719\n",
      "773-th epoch val loss 0.9943058541935711\n",
      "774-th epoch train loss 0.9577683170781744\n",
      "774-th epoch val loss 0.9943059843802874\n",
      "775-th epoch train loss 0.9577684860921036\n",
      "775-th epoch val loss 0.9943061125269206\n",
      "776-th epoch train loss 0.9577686524570146\n",
      "776-th epoch val loss 0.994306238665415\n",
      "777-th epoch train loss 0.9577688162144039\n",
      "777-th epoch val loss 0.9943063628272176\n",
      "778-th epoch train loss 0.9577689774051178\n",
      "778-th epoch val loss 0.9943064850432821\n",
      "779-th epoch train loss 0.9577691360693649\n",
      "779-th epoch val loss 0.9943066053440793\n",
      "780-th epoch train loss 0.957769292246723\n",
      "780-th epoch val loss 0.9943067237596024\n",
      "781-th epoch train loss 0.9577694459761517\n",
      "781-th epoch val loss 0.9943068403193766\n",
      "782-th epoch train loss 0.9577695972960002\n",
      "782-th epoch val loss 0.9943069550524642\n",
      "783-th epoch train loss 0.957769746244018\n",
      "783-th epoch val loss 0.9943070679874748\n",
      "784-th epoch train loss 0.9577698928573635\n",
      "784-th epoch val loss 0.9943071791525683\n",
      "785-th epoch train loss 0.9577700371726138\n",
      "785-th epoch val loss 0.9943072885754658\n",
      "786-th epoch train loss 0.9577701792257725\n",
      "786-th epoch val loss 0.9943073962834537\n",
      "787-th epoch train loss 0.9577703190522809\n",
      "787-th epoch val loss 0.9943075023033924\n",
      "788-th epoch train loss 0.9577704566870262\n",
      "788-th epoch val loss 0.9943076066617215\n",
      "789-th epoch train loss 0.9577705921643469\n",
      "789-th epoch val loss 0.994307709384467\n",
      "790-th epoch train loss 0.9577707255180467\n",
      "790-th epoch val loss 0.9943078104972491\n",
      "791-th epoch train loss 0.9577708567813978\n",
      "791-th epoch val loss 0.9943079100252845\n",
      "792-th epoch train loss 0.9577709859871535\n",
      "792-th epoch val loss 0.9943080079933979\n",
      "793-th epoch train loss 0.9577711131675528\n",
      "793-th epoch val loss 0.994308104426024\n",
      "794-th epoch train loss 0.9577712383543314\n",
      "794-th epoch val loss 0.9943081993472158\n",
      "795-th epoch train loss 0.9577713615787274\n",
      "795-th epoch val loss 0.9943082927806504\n",
      "796-th epoch train loss 0.9577714828714893\n",
      "796-th epoch val loss 0.994308384749633\n",
      "797-th epoch train loss 0.9577716022628855\n",
      "797-th epoch val loss 0.9943084752771058\n",
      "798-th epoch train loss 0.957771719782709\n",
      "798-th epoch val loss 0.9943085643856501\n",
      "799-th epoch train loss 0.9577718354602885\n",
      "799-th epoch val loss 0.9943086520974966\n",
      "800-th epoch train loss 0.9577719493244905\n",
      "800-th epoch val loss 0.9943087384345246\n",
      "801-th epoch train loss 0.9577720614037314\n",
      "801-th epoch val loss 0.9943088234182735\n",
      "802-th epoch train loss 0.9577721717259828\n",
      "802-th epoch val loss 0.9943089070699446\n",
      "803-th epoch train loss 0.9577722803187775\n",
      "803-th epoch val loss 0.9943089894104077\n",
      "804-th epoch train loss 0.9577723872092179\n",
      "804-th epoch val loss 0.994309070460206\n",
      "805-th epoch train loss 0.9577724924239809\n",
      "805-th epoch val loss 0.9943091502395605\n",
      "806-th epoch train loss 0.9577725959893265\n",
      "806-th epoch val loss 0.9943092287683767\n",
      "807-th epoch train loss 0.9577726979311036\n",
      "807-th epoch val loss 0.9943093060662472\n",
      "808-th epoch train loss 0.9577727982747555\n",
      "808-th epoch val loss 0.9943093821524593\n",
      "809-th epoch train loss 0.9577728970453274\n",
      "809-th epoch val loss 0.9943094570459974\n",
      "810-th epoch train loss 0.9577729942674732\n",
      "810-th epoch val loss 0.9943095307655493\n",
      "811-th epoch train loss 0.9577730899654584\n",
      "811-th epoch val loss 0.9943096033295095\n",
      "812-th epoch train loss 0.9577731841631706\n",
      "812-th epoch val loss 0.9943096747559849\n",
      "813-th epoch train loss 0.9577732768841223\n",
      "813-th epoch val loss 0.9943097450627991\n",
      "814-th epoch train loss 0.9577733681514585\n",
      "814-th epoch val loss 0.9943098142674973\n",
      "815-th epoch train loss 0.9577734579879611\n",
      "815-th epoch val loss 0.9943098823873487\n",
      "816-th epoch train loss 0.9577735464160542\n",
      "816-th epoch val loss 0.9943099494393521\n",
      "817-th epoch train loss 0.9577736334578126\n",
      "817-th epoch val loss 0.9943100154402417\n",
      "818-th epoch train loss 0.9577737191349642\n",
      "818-th epoch val loss 0.9943100804064875\n",
      "819-th epoch train loss 0.9577738034688968\n",
      "819-th epoch val loss 0.9943101443543042\n",
      "820-th epoch train loss 0.9577738864806629\n",
      "820-th epoch val loss 0.9943102072996499\n",
      "821-th epoch train loss 0.9577739681909848\n",
      "821-th epoch val loss 0.9943102692582344\n",
      "822-th epoch train loss 0.9577740486202618\n",
      "822-th epoch val loss 0.994310330245521\n",
      "823-th epoch train loss 0.9577741277885727\n",
      "823-th epoch val loss 0.994310390276732\n",
      "824-th epoch train loss 0.9577742057156813\n",
      "824-th epoch val loss 0.9943104493668494\n",
      "825-th epoch train loss 0.9577742824210422\n",
      "825-th epoch val loss 0.9943105075306219\n",
      "826-th epoch train loss 0.9577743579238046\n",
      "826-th epoch val loss 0.9943105647825656\n",
      "827-th epoch train loss 0.9577744322428192\n",
      "827-th epoch val loss 0.9943106211369721\n",
      "828-th epoch train loss 0.9577745053966407\n",
      "828-th epoch val loss 0.994310676607907\n",
      "829-th epoch train loss 0.9577745774035321\n",
      "829-th epoch val loss 0.9943107312092161\n",
      "830-th epoch train loss 0.9577746482814715\n",
      "830-th epoch val loss 0.9943107849545282\n",
      "831-th epoch train loss 0.9577747180481551\n",
      "831-th epoch val loss 0.9943108378572592\n",
      "832-th epoch train loss 0.9577747867210007\n",
      "832-th epoch val loss 0.9943108899306139\n",
      "833-th epoch train loss 0.9577748543171556\n",
      "833-th epoch val loss 0.9943109411875919\n",
      "834-th epoch train loss 0.9577749208534959\n",
      "834-th epoch val loss 0.9943109916409869\n",
      "835-th epoch train loss 0.9577749863466347\n",
      "835-th epoch val loss 0.9943110413033948\n",
      "836-th epoch train loss 0.957775050812925\n",
      "836-th epoch val loss 0.9943110901872116\n",
      "837-th epoch train loss 0.9577751142684623\n",
      "837-th epoch val loss 0.9943111383046411\n",
      "838-th epoch train loss 0.957775176729091\n",
      "838-th epoch val loss 0.994311185667695\n",
      "839-th epoch train loss 0.957775238210407\n",
      "839-th epoch val loss 0.9943112322881977\n",
      "840-th epoch train loss 0.9577752987277617\n",
      "840-th epoch val loss 0.9943112781777865\n",
      "841-th epoch train loss 0.9577753582962655\n",
      "841-th epoch val loss 0.9943113233479186\n",
      "842-th epoch train loss 0.9577754169307925\n",
      "842-th epoch val loss 0.9943113678098696\n",
      "843-th epoch train loss 0.9577754746459846\n",
      "843-th epoch val loss 0.994311411574741\n",
      "844-th epoch train loss 0.9577755314562528\n",
      "844-th epoch val loss 0.9943114546534583\n",
      "845-th epoch train loss 0.9577755873757829\n",
      "845-th epoch val loss 0.9943114970567766\n",
      "846-th epoch train loss 0.9577756424185383\n",
      "846-th epoch val loss 0.9943115387952829\n",
      "847-th epoch train loss 0.9577756965982639\n",
      "847-th epoch val loss 0.9943115798793966\n",
      "848-th epoch train loss 0.9577757499284881\n",
      "848-th epoch val loss 0.9943116203193755\n",
      "849-th epoch train loss 0.9577758024225295\n",
      "849-th epoch val loss 0.9943116601253174\n",
      "850-th epoch train loss 0.9577758540934962\n",
      "850-th epoch val loss 0.9943116993071599\n",
      "851-th epoch train loss 0.9577759049542903\n",
      "851-th epoch val loss 0.9943117378746855\n",
      "852-th epoch train loss 0.957775955017614\n",
      "852-th epoch val loss 0.9943117758375241\n",
      "853-th epoch train loss 0.9577760042959681\n",
      "853-th epoch val loss 0.9943118132051537\n",
      "854-th epoch train loss 0.9577760528016598\n",
      "854-th epoch val loss 0.994311849986906\n",
      "855-th epoch train loss 0.9577761005468011\n",
      "855-th epoch val loss 0.9943118861919636\n",
      "856-th epoch train loss 0.9577761475433156\n",
      "856-th epoch val loss 0.9943119218293669\n",
      "857-th epoch train loss 0.95777619380294\n",
      "857-th epoch val loss 0.9943119569080143\n",
      "858-th epoch train loss 0.9577762393372262\n",
      "858-th epoch val loss 0.9943119914366644\n",
      "859-th epoch train loss 0.9577762841575461\n",
      "859-th epoch val loss 0.9943120254239395\n",
      "860-th epoch train loss 0.9577763282750923\n",
      "860-th epoch val loss 0.9943120588783253\n",
      "861-th epoch train loss 0.9577763717008831\n",
      "861-th epoch val loss 0.9943120918081761\n",
      "862-th epoch train loss 0.9577764144457634\n",
      "862-th epoch val loss 0.9943121242217147\n",
      "863-th epoch train loss 0.9577764565204081\n",
      "863-th epoch val loss 0.9943121561270345\n",
      "864-th epoch train loss 0.9577764979353253\n",
      "864-th epoch val loss 0.994312187532103\n",
      "865-th epoch train loss 0.9577765387008575\n",
      "865-th epoch val loss 0.9943122184447619\n",
      "866-th epoch train loss 0.957776578827186\n",
      "866-th epoch val loss 0.9943122488727302\n",
      "867-th epoch train loss 0.9577766183243325\n",
      "867-th epoch val loss 0.9943122788236071\n",
      "868-th epoch train loss 0.9577766572021612\n",
      "868-th epoch val loss 0.9943123083048712\n",
      "869-th epoch train loss 0.957776695470381\n",
      "869-th epoch val loss 0.9943123373238848\n",
      "870-th epoch train loss 0.9577767331385505\n",
      "870-th epoch val loss 0.9943123658878944\n",
      "871-th epoch train loss 0.9577767702160765\n",
      "871-th epoch val loss 0.994312394004033\n",
      "872-th epoch train loss 0.9577768067122202\n",
      "872-th epoch val loss 0.994312421679322\n",
      "873-th epoch train loss 0.9577768426360961\n",
      "873-th epoch val loss 0.9943124489206724\n",
      "874-th epoch train loss 0.9577768779966763\n",
      "874-th epoch val loss 0.9943124757348881\n",
      "875-th epoch train loss 0.9577769128027928\n",
      "875-th epoch val loss 0.9943125021286642\n",
      "876-th epoch train loss 0.9577769470631385\n",
      "876-th epoch val loss 0.9943125281085925\n",
      "877-th epoch train loss 0.9577769807862702\n",
      "877-th epoch val loss 0.994312553681161\n",
      "878-th epoch train loss 0.9577770139806109\n",
      "878-th epoch val loss 0.9943125788527561\n",
      "879-th epoch train loss 0.9577770466544513\n",
      "879-th epoch val loss 0.9943126036296649\n",
      "880-th epoch train loss 0.957777078815952\n",
      "880-th epoch val loss 0.994312628018074\n",
      "881-th epoch train loss 0.9577771104731464\n",
      "881-th epoch val loss 0.9943126520240748\n",
      "882-th epoch train loss 0.95777714163394\n",
      "882-th epoch val loss 0.994312675653662\n",
      "883-th epoch train loss 0.957777172306118\n",
      "883-th epoch val loss 0.994312698912738\n",
      "884-th epoch train loss 0.9577772024973393\n",
      "884-th epoch val loss 0.9943127218071107\n",
      "885-th epoch train loss 0.9577772322151459\n",
      "885-th epoch val loss 0.9943127443424983\n",
      "886-th epoch train loss 0.9577772614669604\n",
      "886-th epoch val loss 0.9943127665245289\n",
      "887-th epoch train loss 0.9577772902600886\n",
      "887-th epoch val loss 0.9943127883587426\n",
      "888-th epoch train loss 0.9577773186017223\n",
      "888-th epoch val loss 0.994312809850592\n",
      "889-th epoch train loss 0.9577773464989419\n",
      "889-th epoch val loss 0.9943128310054458\n",
      "890-th epoch train loss 0.9577773739587139\n",
      "890-th epoch val loss 0.9943128518285869\n",
      "891-th epoch train loss 0.9577774009878973\n",
      "891-th epoch val loss 0.9943128723252153\n",
      "892-th epoch train loss 0.9577774275932442\n",
      "892-th epoch val loss 0.9943128925004513\n",
      "893-th epoch train loss 0.9577774537814001\n",
      "893-th epoch val loss 0.9943129123593337\n",
      "894-th epoch train loss 0.9577774795589054\n",
      "894-th epoch val loss 0.9943129319068222\n",
      "895-th epoch train loss 0.9577775049321994\n",
      "895-th epoch val loss 0.9943129511477989\n",
      "896-th epoch train loss 0.9577775299076204\n",
      "896-th epoch val loss 0.9943129700870701\n",
      "897-th epoch train loss 0.9577775544914059\n",
      "897-th epoch val loss 0.9943129887293651\n",
      "898-th epoch train loss 0.9577775786896972\n",
      "898-th epoch val loss 0.994313007079341\n",
      "899-th epoch train loss 0.957777602508538\n",
      "899-th epoch val loss 0.9943130251415805\n",
      "900-th epoch train loss 0.9577776259538795\n",
      "900-th epoch val loss 0.9943130429205955\n",
      "901-th epoch train loss 0.9577776490315759\n",
      "901-th epoch val loss 0.9943130604208258\n",
      "902-th epoch train loss 0.9577776717473936\n",
      "902-th epoch val loss 0.9943130776466435\n",
      "903-th epoch train loss 0.9577776941070051\n",
      "903-th epoch val loss 0.9943130946023501\n",
      "904-th epoch train loss 0.9577777161159978\n",
      "904-th epoch val loss 0.9943131112921818\n",
      "905-th epoch train loss 0.9577777377798671\n",
      "905-th epoch val loss 0.9943131277203062\n",
      "906-th epoch train loss 0.9577777591040263\n",
      "906-th epoch val loss 0.9943131438908275\n",
      "907-th epoch train loss 0.9577777800938007\n",
      "907-th epoch val loss 0.9943131598077842\n",
      "908-th epoch train loss 0.9577778007544334\n",
      "908-th epoch val loss 0.9943131754751515\n",
      "909-th epoch train loss 0.9577778210910868\n",
      "909-th epoch val loss 0.9943131908968439\n",
      "910-th epoch train loss 0.9577778411088393\n",
      "910-th epoch val loss 0.9943132060767128\n",
      "911-th epoch train loss 0.9577778608126924\n",
      "911-th epoch val loss 0.9943132210185501\n",
      "912-th epoch train loss 0.9577778802075673\n",
      "912-th epoch val loss 0.9943132357260877\n",
      "913-th epoch train loss 0.9577778992983088\n",
      "913-th epoch val loss 0.9943132502029995\n",
      "914-th epoch train loss 0.9577779180896862\n",
      "914-th epoch val loss 0.9943132644529015\n",
      "915-th epoch train loss 0.9577779365863935\n",
      "915-th epoch val loss 0.9943132784793529\n",
      "916-th epoch train loss 0.9577779547930514\n",
      "916-th epoch val loss 0.9943132922858583\n",
      "917-th epoch train loss 0.9577779727142074\n",
      "917-th epoch val loss 0.9943133058758653\n",
      "918-th epoch train loss 0.9577779903543384\n",
      "918-th epoch val loss 0.9943133192527687\n",
      "919-th epoch train loss 0.9577780077178512\n",
      "919-th epoch val loss 0.99431333241991\n",
      "920-th epoch train loss 0.9577780248090826\n",
      "920-th epoch val loss 0.9943133453805786\n",
      "921-th epoch train loss 0.957778041632303\n",
      "921-th epoch val loss 0.9943133581380114\n",
      "922-th epoch train loss 0.9577780581917142\n",
      "922-th epoch val loss 0.9943133706953956\n",
      "923-th epoch train loss 0.9577780744914532\n",
      "923-th epoch val loss 0.9943133830558671\n",
      "924-th epoch train loss 0.9577780905355919\n",
      "924-th epoch val loss 0.9943133952225145\n",
      "925-th epoch train loss 0.9577781063281373\n",
      "925-th epoch val loss 0.9943134071983764\n",
      "926-th epoch train loss 0.9577781218730358\n",
      "926-th epoch val loss 0.9943134189864441\n",
      "927-th epoch train loss 0.9577781371741695\n",
      "927-th epoch val loss 0.9943134305896623\n",
      "928-th epoch train loss 0.9577781522353612\n",
      "928-th epoch val loss 0.9943134420109299\n",
      "929-th epoch train loss 0.9577781670603739\n",
      "929-th epoch val loss 0.9943134532530996\n",
      "930-th epoch train loss 0.957778181652911\n",
      "930-th epoch val loss 0.9943134643189805\n",
      "931-th epoch train loss 0.9577781960166156\n",
      "931-th epoch val loss 0.9943134752113343\n",
      "932-th epoch train loss 0.9577782101550789\n",
      "932-th epoch val loss 0.9943134859328846\n",
      "933-th epoch train loss 0.9577782240718312\n",
      "933-th epoch val loss 0.9943134964863087\n",
      "934-th epoch train loss 0.9577782377703496\n",
      "934-th epoch val loss 0.9943135068742426\n",
      "935-th epoch train loss 0.9577782512540564\n",
      "935-th epoch val loss 0.9943135170992817\n",
      "936-th epoch train loss 0.9577782645263191\n",
      "936-th epoch val loss 0.9943135271639797\n",
      "937-th epoch train loss 0.9577782775904539\n",
      "937-th epoch val loss 0.9943135370708512\n",
      "938-th epoch train loss 0.9577782904497245\n",
      "938-th epoch val loss 0.9943135468223708\n",
      "939-th epoch train loss 0.9577783031073427\n",
      "939-th epoch val loss 0.9943135564209743\n",
      "940-th epoch train loss 0.9577783155664716\n",
      "940-th epoch val loss 0.99431356586906\n",
      "941-th epoch train loss 0.9577783278302227\n",
      "941-th epoch val loss 0.9943135751689874\n",
      "942-th epoch train loss 0.9577783399016602\n",
      "942-th epoch val loss 0.9943135843230806\n",
      "943-th epoch train loss 0.9577783517838001\n",
      "943-th epoch val loss 0.9943135933336255\n",
      "944-th epoch train loss 0.9577783634796095\n",
      "944-th epoch val loss 0.9943136022028733\n",
      "945-th epoch train loss 0.957778374992011\n",
      "945-th epoch val loss 0.9943136109330392\n",
      "946-th epoch train loss 0.9577783863238809\n",
      "946-th epoch val loss 0.994313619526305\n",
      "947-th epoch train loss 0.9577783974780496\n",
      "947-th epoch val loss 0.9943136279848168\n",
      "948-th epoch train loss 0.9577784084573042\n",
      "948-th epoch val loss 0.9943136363106876\n",
      "949-th epoch train loss 0.9577784192643867\n",
      "949-th epoch val loss 0.9943136445059976\n",
      "950-th epoch train loss 0.9577784299019979\n",
      "950-th epoch val loss 0.9943136525727937\n",
      "951-th epoch train loss 0.9577784403727947\n",
      "951-th epoch val loss 0.9943136605130914\n",
      "952-th epoch train loss 0.957778450679393\n",
      "952-th epoch val loss 0.9943136683288742\n",
      "953-th epoch train loss 0.9577784608243679\n",
      "953-th epoch val loss 0.9943136760220949\n",
      "954-th epoch train loss 0.9577784708102529\n",
      "954-th epoch val loss 0.9943136835946743\n",
      "955-th epoch train loss 0.9577784806395435\n",
      "955-th epoch val loss 0.994313691048505\n",
      "956-th epoch train loss 0.9577784903146951\n",
      "956-th epoch val loss 0.9943136983854487\n",
      "957-th epoch train loss 0.9577784998381248\n",
      "957-th epoch val loss 0.9943137056073387\n",
      "958-th epoch train loss 0.9577785092122117\n",
      "958-th epoch val loss 0.9943137127159791\n",
      "959-th epoch train loss 0.9577785184392974\n",
      "959-th epoch val loss 0.9943137197131449\n",
      "960-th epoch train loss 0.9577785275216875\n",
      "960-th epoch val loss 0.9943137266005853\n",
      "961-th epoch train loss 0.9577785364616511\n",
      "961-th epoch val loss 0.9943137333800199\n",
      "962-th epoch train loss 0.9577785452614214\n",
      "962-th epoch val loss 0.9943137400531433\n",
      "963-th epoch train loss 0.9577785539231966\n",
      "963-th epoch val loss 0.9943137466216219\n",
      "964-th epoch train loss 0.9577785624491402\n",
      "964-th epoch val loss 0.9943137530870967\n",
      "965-th epoch train loss 0.9577785708413837\n",
      "965-th epoch val loss 0.994313759451183\n",
      "966-th epoch train loss 0.9577785791020225\n",
      "966-th epoch val loss 0.9943137657154707\n",
      "967-th epoch train loss 0.9577785872331199\n",
      "967-th epoch val loss 0.9943137718815244\n",
      "968-th epoch train loss 0.9577785952367088\n",
      "968-th epoch val loss 0.9943137779508847\n",
      "969-th epoch train loss 0.9577786031147874\n",
      "969-th epoch val loss 0.9943137839250681\n",
      "970-th epoch train loss 0.9577786108693245\n",
      "970-th epoch val loss 0.9943137898055672\n",
      "971-th epoch train loss 0.9577786185022572\n",
      "971-th epoch val loss 0.9943137955938506\n",
      "972-th epoch train loss 0.9577786260154925\n",
      "972-th epoch val loss 0.9943138012913642\n",
      "973-th epoch train loss 0.957778633410907\n",
      "973-th epoch val loss 0.994313806899532\n",
      "974-th epoch train loss 0.9577786406903491\n",
      "974-th epoch val loss 0.9943138124197549\n",
      "975-th epoch train loss 0.9577786478556365\n",
      "975-th epoch val loss 0.9943138178534112\n",
      "976-th epoch train loss 0.95777865490856\n",
      "976-th epoch val loss 0.9943138232018591\n",
      "977-th epoch train loss 0.9577786618508806\n",
      "977-th epoch val loss 0.9943138284664342\n",
      "978-th epoch train loss 0.9577786686843346\n",
      "978-th epoch val loss 0.9943138336484533\n",
      "979-th epoch train loss 0.9577786754106279\n",
      "979-th epoch val loss 0.9943138387492093\n",
      "980-th epoch train loss 0.9577786820314403\n",
      "980-th epoch val loss 0.9943138437699767\n",
      "981-th epoch train loss 0.9577786885484272\n",
      "981-th epoch val loss 0.9943138487120105\n",
      "982-th epoch train loss 0.9577786949632163\n",
      "982-th epoch val loss 0.9943138535765449\n",
      "983-th epoch train loss 0.9577787012774097\n",
      "983-th epoch val loss 0.9943138583647951\n",
      "984-th epoch train loss 0.9577787074925856\n",
      "984-th epoch val loss 0.9943138630779578\n",
      "985-th epoch train loss 0.9577787136102963\n",
      "985-th epoch val loss 0.9943138677172096\n",
      "986-th epoch train loss 0.9577787196320703\n",
      "986-th epoch val loss 0.9943138722837106\n",
      "987-th epoch train loss 0.9577787255594123\n",
      "987-th epoch val loss 0.9943138767786012\n",
      "988-th epoch train loss 0.9577787313938025\n",
      "988-th epoch val loss 0.9943138812030037\n",
      "989-th epoch train loss 0.9577787371366991\n",
      "989-th epoch val loss 0.994313885558024\n",
      "990-th epoch train loss 0.9577787427895368\n",
      "990-th epoch val loss 0.9943138898447506\n",
      "991-th epoch train loss 0.9577787483537277\n",
      "991-th epoch val loss 0.9943138940642536\n",
      "992-th epoch train loss 0.9577787538306619\n",
      "992-th epoch val loss 0.994313898217588\n",
      "993-th epoch train loss 0.9577787592217077\n",
      "993-th epoch val loss 0.99431390230579\n",
      "994-th epoch train loss 0.957778764528212\n",
      "994-th epoch val loss 0.994313906329882\n",
      "995-th epoch train loss 0.9577787697515004\n",
      "995-th epoch val loss 0.9943139102908697\n",
      "996-th epoch train loss 0.957778774892878\n",
      "996-th epoch val loss 0.9943139141897417\n",
      "997-th epoch train loss 0.9577787799536299\n",
      "997-th epoch val loss 0.9943139180274729\n",
      "998-th epoch train loss 0.957778784935019\n",
      "998-th epoch val loss 0.9943139218050214\n",
      "999-th epoch train loss 0.9577787898382915\n",
      "999-th epoch val loss 0.9943139255233316\n"
     ]
    }
   ],
   "source": [
    "slr = ScratchLinearRegression(num_iter=1000, lr=0.01, no_bias=True, verbose=True)\n",
    "slr.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0674ce8a-7147-4ba5-b76c-15f23ca45639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2239f5f5a80>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(slr.loss)\n",
    "plt.plot(slr.val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6f22c119-effb-43b1-95dc-915638939310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    mse = np.mean((y-y_pred)**2)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c9241061-2137-4994-91cd-cef1945802ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = slr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2acb1973-a07e-4010-b005-e2b887799e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55102879930.74813"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(np.exp(pred_test), np.exp(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e6332-876f-4484-93e0-4bb8a435cce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
