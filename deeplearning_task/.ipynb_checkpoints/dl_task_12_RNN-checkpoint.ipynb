{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "-35KBug0IJCc"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "xguVL7aWIJCf"
      },
      "outputs": [],
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray, shape (n_samples, n_features)\n",
        "    y : ndarray, shape (n_samples, 1)\n",
        "\n",
        "    batch_size : int\n",
        "    seed : int\n",
        "      NumPy\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "MSrTNYdJIJCg"
      },
      "outputs": [],
      "source": [
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma = 0.01):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
        "        return B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "SXzMOoriIJCh"
      },
      "outputs": [],
      "source": [
        "class FC:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "    n_nodes2 : int\n",
        "    initializer\n",
        "    optimizer\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, dropout_rate=0.5):\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "        self.dZ = 0\n",
        "        self.dA = 0\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.mask = None\n",
        "        self.input_X_forward = 0\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        self.input_X_forward = X\n",
        "        A = np.dot(X, self.W) + self.B\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "\n",
        "        dW = np.dot(self.input_X_forward.T, dA)\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "        self.dA = dA\n",
        "        self.dW = dW\n",
        "        self.dZ = dZ\n",
        "        self = self.optimizer.update(self)\n",
        "        return dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "03wE8jdqIJCh"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "\n",
        "        layer.B = layer.B - self.lr * np.average(layer.dA, axis=0)\n",
        "        layer.W = layer.W - self.lr * layer.dW / layer.dA.shape[0]\n",
        "\n",
        "        return layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "SpcqCD0gIJCh"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.input_X_forward = 0\n",
        "\n",
        "    def _func(self, X):\n",
        "        return 1 / (1 + np.exp(-1 * X))\n",
        "\n",
        "    def _func_diff(self, X):\n",
        "        return (1 - self._func(X)) * self._func(X)\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        self.input_X_forward = X\n",
        "        A = self._func(X)\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "\n",
        "        grad = self._func_diff(self.input_X_forward)\n",
        "        dZ = grad * dA\n",
        "        return dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "miru0ldUIJCi"
      },
      "outputs": [],
      "source": [
        "class Tanh:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.input_X_forward = 0\n",
        "\n",
        "    def _func(self, X):\n",
        "        return np.tanh(X)\n",
        "\n",
        "    def _func_diff(self, X):\n",
        "        return 1 - (self._func(X))**2\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        self.input_X_forward = X\n",
        "        A = self._func(X)\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "\n",
        "        grad = self._func_diff(self.input_X_forward)\n",
        "        dZ = grad * dA\n",
        "        return dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "bRaztZ7VIJCi"
      },
      "outputs": [],
      "source": [
        "class softmax:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.input_X_forward = 0\n",
        "        self.pred = 0\n",
        "\n",
        "    def _func(self, X):\n",
        "        X = X - np.max(X)\n",
        "        tmp = np.exp(X)\n",
        "        denominator = np.sum(tmp, axis=1)\n",
        "        output = tmp / denominator[:, np.newaxis]\n",
        "        return output\n",
        "\n",
        "    def _func_diff(self, X):\n",
        "        return X\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        self.input_X_forward = X\n",
        "        A = self._func(X)\n",
        "        self.pred = A\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "\n",
        "        dZ = self.pred - dA\n",
        "\n",
        "        return dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "GXBvjJywIJCj"
      },
      "outputs": [],
      "source": [
        "class ReLU:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.input_X_forward = 0\n",
        "\n",
        "    def _func(self, X):\n",
        "        return np.maximum(0, X)\n",
        "\n",
        "    def _func_diff(self, X):\n",
        "        return np.where( x > 0, 1, 0)\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        self.input_X_forward = X\n",
        "        A = self._func(X)\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "\n",
        "        grad = self._func_diff(self.input_X_forward)\n",
        "        dZ = grad * dA\n",
        "        return dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "id": "bl6kDj6QIJCj"
      },
      "outputs": [],
      "source": [
        "class XavierInitializer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_prev_nodes = 1\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.n_prev_nodes = n_nodes1\n",
        "        W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        B = np.random.randn(1, n_nodes2) / np.sqrt(self.n_prev_nodes)\n",
        "        return B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "Y-K9VmRjIJCk"
      },
      "outputs": [],
      "source": [
        "class HeInitializer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_prev_nodes = 1\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.n_prev_nodes = n_nodes1\n",
        "        W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        B = np.random.randn(1, n_nodes2) * np.sqrt(2 / self.n_prev_nodes)\n",
        "        return B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "AFoYLsSsIJCk"
      },
      "outputs": [],
      "source": [
        "class AdaGrad:\n",
        "\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.H_B = 1\n",
        "        self.H_W = 1\n",
        "    def update(self, layer):\n",
        "\n",
        "        self.H_B = self.H_B + np.average(layer.dA)**2\n",
        "        self.H_W = self.H_W + np.average(layer.dW)**2\n",
        "\n",
        "        layer.B = layer.B - self.lr * np.average(layer.dA, axis=0) / np.sqrt(self.H_B)\n",
        "        layer.W = layer.W - self.lr * layer.dW / layer.dA.shape[0] / np.sqrt(self.H_W)\n",
        "\n",
        "        return layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTjY5oizIJCk"
      },
      "source": [
        "# [Problem 1] Forward propagation implementation of SimpleRNN\n",
        "Create a SimpleRNN class SimpleRNN. The basic structure will be the same as the FC class.\n",
        "\n",
        "The forward propagation formula looks like this: It also describes what the shape of ndarray will be.\n",
        "\n",
        "We denote the batch size batch_size, the number of input features n_features, and the number of RNN nodes . n_nodesThe activation function proceeds as tanh, but it can be replaced with ReLU, etc., as in previous neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UzCZ84n_IJCm"
      },
      "outputs": [],
      "source": [
        "class SimpleRNN:\n",
        "\n",
        "    def __init__(self, W_x, B_x, W_h, initializer, optimizer, activation):\n",
        "        self.optimizer = optimizer\n",
        "        # 初期化\n",
        "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
        "        #self.W1 = initializer.W(n_wx_nodes1, n_wx_nodes2)\n",
        "        #self.B1 = initializer.B(1)\n",
        "        self.Wx = W_x\n",
        "        self.Bx = B_x\n",
        "        self.Wh = W_h\n",
        "        self.dA = 0\n",
        "        self.dW = 0\n",
        "        self.W = 0\n",
        "        self.B = 0\n",
        "        self.input_X_forward = 0\n",
        "        self.input_prev_ht_forward = 0\n",
        "        self.activation = activation\n",
        "        self.n_sequece = 0\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        self.input_X_forward = X\n",
        "        self.n_sequece = X.shape[1]\n",
        "        tmp_prev_h = np.zeros((X.shape[1]+1, X.shape[0], self.Wx.shape[1]))\n",
        "        self.input_prev_ht_forward = np.zeros((X.shape[0], X.shape[1], self.Wx.shape[1]))\n",
        "        y = np.zeros((X.shape[0], X.shape[1], self.Wx.shape[1]))\n",
        "        tmp_y = np.zeros((X.shape[1], X.shape[0], self.Wx.shape[1]))\n",
        "        for i in range(self.n_sequece):\n",
        "            Xt = X[:,i]\n",
        "            #Xt:(batch, Feature)\n",
        "            tmp = np.dot(Xt, self.Wx) + self.Bx + tmp_prev_h[i]\n",
        "            #tmp:(batch, Node1)\n",
        "            tmp_y[i] = self.activation.forward(tmp)\n",
        "            #h_prev:(batch, node2)\n",
        "            tmp_prev_h[i+1] = np.dot(tmp_y[i], self.Wh)\n",
        "\n",
        "        self.input_prev_ht_forward = tmp_prev_h.transpose(1,0,2)\n",
        "        y = tmp_y.transpose(1,0,2)\n",
        "        return y\n",
        "\n",
        "    def backward(self, dA):\n",
        "\n",
        "        dz = np.zeros_like(self.input_X_forward)\n",
        "        tmp_dz = dz.transpose(1,0,2)\n",
        "\n",
        "        loss_h = np.zeros((dA.shape[0], dA.shape[1]+1, dA.shape[2]))\n",
        "        for i in reversed(range(self.n_sequece)):\n",
        "            loss = dA[:,i,:] + loss_h[:,i,:]\n",
        "            loss = self.activation.backward(loss) * loss\n",
        "            dW = np.dot(self.input_X_forward[:,i,:].T, loss)\n",
        "            tmp_dz[i] = np.dot(loss, self.Wx.T)\n",
        "            self.dA = loss\n",
        "            self.dW = dW\n",
        "            self.W = self.Wx\n",
        "            self.B = self.Bx\n",
        "            self = self.optimizer.update(self)\n",
        "            self.Wx = self.W\n",
        "            self.Bx = self.B\n",
        "\n",
        "            loss_h[:,i+1,:] = np.dot(loss, self.Wh.T)\n",
        "            self.dA = loss\n",
        "            dW = np.dot(self.input_prev_ht_forward[:,i,:].T, loss)\n",
        "            self.dW = dW\n",
        "            self.W = self.Wh\n",
        "            self.B = 0\n",
        "            self = self.optimizer.update(self)\n",
        "            self.Wh = self.W\n",
        "\n",
        "        dz = tmp_dz.transpose(1,0,2)\n",
        "        return dz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvOGFPJyIJCp"
      },
      "source": [
        "# [Problem 2] Forward propagation experiment with small sequences\n",
        "\n",
        "Consider forward propagation on small arrays.\n",
        "Let input x, initial state h, weights w_x and w_h, bias b be:\n",
        "Here the axes of the array x are in order of batch size, number of series, and number of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PNRzjZvuIJCq"
      },
      "outputs": [],
      "source": [
        "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100\n",
        "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100\n",
        "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100\n",
        "batch_size = x.shape[0] # 1\n",
        "n_sequences = x.shape[1] # 3\n",
        "n_features = x.shape[2] # 2\n",
        "n_nodes = w_x.shape[1] # 4\n",
        "h = np.zeros((batch_size, n_nodes))\n",
        "b = np.array([1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EO370ZgMIJCq"
      },
      "outputs": [],
      "source": [
        "rnn = SimpleRNN(w_x, 1, w_h, initializer=SimpleInitializer(), optimizer=SGD(0.01), activation=Tanh())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "O6Mxg3IXIJCq"
      },
      "outputs": [],
      "source": [
        "h = rnn.forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "144pLqb2IJCr",
        "outputId": "2eabcedf-6132-4962-a049-bcdc44f4ee54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "h.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5l9_9x9IJCs",
        "outputId": "5b46979b-d048-4a59-ae0c-f70e22f1e77d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.79494228, 0.81839002, 0.83939649, 0.85584174])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "h[0,2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxglzt00IJCt"
      },
      "source": [
        "Result：h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZN593owIJCt"
      },
      "source": [
        "# [Problem 3] (Advanced assignment) Implementing backpropagation\n",
        "\n",
        "Time-trough backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "id": "qz0USOzUIJCt"
      },
      "outputs": [],
      "source": [
        "dA = np.array([[[0.01, 0.02, 0.03, 0.04], [0.01, 0.02, 0.03, 0.04], [0.01, 0.02, 0.03, 0.04]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg-xhDM8IJCt",
        "outputId": "44145e3f-bf19-4877-8b2c-f99959a899af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[4.75883037e-05, 6.05642872e-05],\n",
              "        [4.75883582e-05, 6.05643690e-05],\n",
              "        [4.75884400e-05, 6.05644781e-05]]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "rnn.backward(dA)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}